{"cells":[{"cell_type":"markdown","metadata":{"id":"fI00-KuddN2u"},"source":["Mount Google Drive (optional)"]},{"cell_type":"code","execution_count":783,"metadata":{"id":"Uf6ucZcj6kpK"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# import os\n","# os.chdir(\"/content/drive/MyDrive/....\")  # file path\n","# print(os.getcwd())"]},{"cell_type":"markdown","metadata":{"id":"yvRo67Io4NKF"},"source":["# **HW2 : Decision Tree and Random Forest**\n","In *assignment 2*, you need to finish :\n","\n","1. Basic Part : Implement a **Decision Tree** model and predict whether the patients in the validation set have diabetes\n","> * Step 1 : Load the input data\n","> * Step 2 : Calculate the Entropy and Information Gain\n","> * Step 3 : Find the Best Split\n","> * Step 4 : Split into 2 branches\n","> * Step 5 : Build decision tree\n","> * Step 6 : Save the answers from step2 to step5\n","> * Step 7 : Split data into training set and validation set\n","> * Step 8 : Train a decision tree model with training set\n","> * Step 9 : Predict the cases in the *validation set* by using the model trained in *Step8*\n","> * Step 10 : Calculate the f1-score of your predictions in *Step9*\n","> * Step 11 : Write the Output File\n","\n","2. Advanced Part : Build a **Random Forest** model to make predictions\n","> * Step 1 : Load the input data\n","> * Step 2 : Load the test data\n","> * Step 3 : Build a random forest\n","> * Step 4 : Predict the cases in the test data by using the model trained in *Step3*\n","> * Step 5 : Save the predictions(from *Step 4*) in a csv file\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wwVh8lYD4kbV"},"source":["# **Basic Part** (60%)\n","In this part, your need to implement a Decision Tree model by completing the following given functions.\n","\n","Also, you need to run these functions with the given input variables and save the output in a csv file **hw2_basic.csv**"]},{"cell_type":"markdown","metadata":{"id":"h2ibEyDa46X2"},"source":["## Import Packages\n","\n","\n","> Note : You **cannot** import any other packages in both basic part and advanced part\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":784,"metadata":{"id":"RMjaYVZD6kmb"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import math\n","import random\n","from numpy import sqrt\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import accuracy_score"]},{"cell_type":"markdown","metadata":{"id":"zrQXqH475G8-"},"source":["## Step1: Load the input data\n","First, load the input file **hw2_input_basic.csv**"]},{"cell_type":"code","execution_count":785,"metadata":{"id":"0n3gcL2l6kjb"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>bmi</th>\n","      <th>gender</th>\n","      <th>height</th>\n","      <th>weight</th>\n","      <th>glucose_apache</th>\n","      <th>heart_rate_apache</th>\n","      <th>resprate_apache</th>\n","      <th>sodium_apache</th>\n","      <th>diabetes_mellitus</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>70.0</td>\n","      <td>25.984659</td>\n","      <td>1</td>\n","      <td>172.7</td>\n","      <td>77.50</td>\n","      <td>116.0</td>\n","      <td>101.0</td>\n","      <td>49.0</td>\n","      <td>137.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>30.0</td>\n","      <td>31.310368</td>\n","      <td>1</td>\n","      <td>170.2</td>\n","      <td>90.70</td>\n","      <td>71.0</td>\n","      <td>39.0</td>\n","      <td>33.0</td>\n","      <td>144.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>54.0</td>\n","      <td>24.388824</td>\n","      <td>1</td>\n","      <td>177.8</td>\n","      <td>77.10</td>\n","      <td>120.0</td>\n","      <td>120.0</td>\n","      <td>31.0</td>\n","      <td>141.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>65.0</td>\n","      <td>34.141074</td>\n","      <td>0</td>\n","      <td>170.2</td>\n","      <td>98.90</td>\n","      <td>73.0</td>\n","      <td>48.0</td>\n","      <td>36.0</td>\n","      <td>140.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>49.0</td>\n","      <td>22.564743</td>\n","      <td>1</td>\n","      <td>172.7</td>\n","      <td>67.30</td>\n","      <td>207.0</td>\n","      <td>119.0</td>\n","      <td>6.0</td>\n","      <td>144.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>62.0</td>\n","      <td>29.424010</td>\n","      <td>0</td>\n","      <td>154.9</td>\n","      <td>70.60</td>\n","      <td>113.0</td>\n","      <td>60.0</td>\n","      <td>32.0</td>\n","      <td>137.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>85.0</td>\n","      <td>27.673574</td>\n","      <td>1</td>\n","      <td>154.9</td>\n","      <td>66.40</td>\n","      <td>102.0</td>\n","      <td>49.0</td>\n","      <td>36.0</td>\n","      <td>142.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>65.0</td>\n","      <td>22.269432</td>\n","      <td>1</td>\n","      <td>177.8</td>\n","      <td>70.40</td>\n","      <td>333.0</td>\n","      <td>59.0</td>\n","      <td>6.0</td>\n","      <td>145.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>85.0</td>\n","      <td>35.879362</td>\n","      <td>0</td>\n","      <td>165.1</td>\n","      <td>97.80</td>\n","      <td>124.0</td>\n","      <td>92.0</td>\n","      <td>30.0</td>\n","      <td>136.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>81.0</td>\n","      <td>20.859375</td>\n","      <td>0</td>\n","      <td>160.0</td>\n","      <td>53.40</td>\n","      <td>136.0</td>\n","      <td>118.0</td>\n","      <td>52.0</td>\n","      <td>138.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>59.0</td>\n","      <td>46.409136</td>\n","      <td>0</td>\n","      <td>162.6</td>\n","      <td>122.70</td>\n","      <td>169.0</td>\n","      <td>100.0</td>\n","      <td>46.0</td>\n","      <td>138.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>77.0</td>\n","      <td>32.324734</td>\n","      <td>0</td>\n","      <td>154.9</td>\n","      <td>77.56</td>\n","      <td>264.0</td>\n","      <td>90.0</td>\n","      <td>37.0</td>\n","      <td>141.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>68.0</td>\n","      <td>15.913579</td>\n","      <td>1</td>\n","      <td>185.4</td>\n","      <td>54.70</td>\n","      <td>39.0</td>\n","      <td>108.0</td>\n","      <td>45.0</td>\n","      <td>135.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>51.0</td>\n","      <td>24.028492</td>\n","      <td>1</td>\n","      <td>190.5</td>\n","      <td>87.20</td>\n","      <td>80.0</td>\n","      <td>61.0</td>\n","      <td>30.0</td>\n","      <td>139.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>76.0</td>\n","      <td>34.216873</td>\n","      <td>0</td>\n","      <td>154.9</td>\n","      <td>82.10</td>\n","      <td>306.0</td>\n","      <td>112.0</td>\n","      <td>40.0</td>\n","      <td>130.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>48.0</td>\n","      <td>26.516476</td>\n","      <td>1</td>\n","      <td>180.3</td>\n","      <td>86.20</td>\n","      <td>96.0</td>\n","      <td>133.0</td>\n","      <td>31.0</td>\n","      <td>137.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>82.0</td>\n","      <td>18.921389</td>\n","      <td>0</td>\n","      <td>154.9</td>\n","      <td>45.40</td>\n","      <td>164.0</td>\n","      <td>103.0</td>\n","      <td>48.0</td>\n","      <td>134.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>78.0</td>\n","      <td>36.668167</td>\n","      <td>0</td>\n","      <td>167.6</td>\n","      <td>103.00</td>\n","      <td>282.0</td>\n","      <td>104.0</td>\n","      <td>42.0</td>\n","      <td>138.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>62.0</td>\n","      <td>19.108088</td>\n","      <td>1</td>\n","      <td>157.5</td>\n","      <td>47.40</td>\n","      <td>275.0</td>\n","      <td>108.0</td>\n","      <td>38.0</td>\n","      <td>131.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>73.0</td>\n","      <td>22.851562</td>\n","      <td>0</td>\n","      <td>160.0</td>\n","      <td>58.50</td>\n","      <td>178.0</td>\n","      <td>154.0</td>\n","      <td>55.0</td>\n","      <td>138.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>73.0</td>\n","      <td>20.351971</td>\n","      <td>0</td>\n","      <td>167.5</td>\n","      <td>57.10</td>\n","      <td>159.0</td>\n","      <td>110.0</td>\n","      <td>27.0</td>\n","      <td>139.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>59.0</td>\n","      <td>27.109238</td>\n","      <td>1</td>\n","      <td>177.8</td>\n","      <td>85.70</td>\n","      <td>85.0</td>\n","      <td>92.0</td>\n","      <td>6.0</td>\n","      <td>143.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>51.0</td>\n","      <td>37.954367</td>\n","      <td>0</td>\n","      <td>172.7</td>\n","      <td>113.20</td>\n","      <td>168.0</td>\n","      <td>60.0</td>\n","      <td>43.0</td>\n","      <td>119.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>80.0</td>\n","      <td>36.267895</td>\n","      <td>1</td>\n","      <td>180.3</td>\n","      <td>117.90</td>\n","      <td>138.0</td>\n","      <td>178.0</td>\n","      <td>43.0</td>\n","      <td>140.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>74.0</td>\n","      <td>25.856081</td>\n","      <td>1</td>\n","      <td>170.2</td>\n","      <td>74.90</td>\n","      <td>145.0</td>\n","      <td>40.0</td>\n","      <td>16.0</td>\n","      <td>146.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>61.0</td>\n","      <td>29.161972</td>\n","      <td>1</td>\n","      <td>180.3</td>\n","      <td>94.80</td>\n","      <td>267.0</td>\n","      <td>62.0</td>\n","      <td>58.0</td>\n","      <td>134.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>74.0</td>\n","      <td>38.111136</td>\n","      <td>1</td>\n","      <td>188.0</td>\n","      <td>134.70</td>\n","      <td>279.0</td>\n","      <td>112.0</td>\n","      <td>43.0</td>\n","      <td>132.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>65.0</td>\n","      <td>53.414791</td>\n","      <td>0</td>\n","      <td>166.4</td>\n","      <td>147.90</td>\n","      <td>121.0</td>\n","      <td>88.0</td>\n","      <td>32.0</td>\n","      <td>142.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>38.0</td>\n","      <td>33.438787</td>\n","      <td>0</td>\n","      <td>161.3</td>\n","      <td>87.00</td>\n","      <td>135.0</td>\n","      <td>46.0</td>\n","      <td>28.0</td>\n","      <td>138.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>82.0</td>\n","      <td>32.263238</td>\n","      <td>0</td>\n","      <td>162.6</td>\n","      <td>85.30</td>\n","      <td>111.0</td>\n","      <td>114.0</td>\n","      <td>22.0</td>\n","      <td>143.0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     age        bmi  gender  height  weight  glucose_apache  \\\n","0   70.0  25.984659       1   172.7   77.50           116.0   \n","1   30.0  31.310368       1   170.2   90.70            71.0   \n","2   54.0  24.388824       1   177.8   77.10           120.0   \n","3   65.0  34.141074       0   170.2   98.90            73.0   \n","4   49.0  22.564743       1   172.7   67.30           207.0   \n","5   62.0  29.424010       0   154.9   70.60           113.0   \n","6   85.0  27.673574       1   154.9   66.40           102.0   \n","7   65.0  22.269432       1   177.8   70.40           333.0   \n","8   85.0  35.879362       0   165.1   97.80           124.0   \n","9   81.0  20.859375       0   160.0   53.40           136.0   \n","10  59.0  46.409136       0   162.6  122.70           169.0   \n","11  77.0  32.324734       0   154.9   77.56           264.0   \n","12  68.0  15.913579       1   185.4   54.70            39.0   \n","13  51.0  24.028492       1   190.5   87.20            80.0   \n","14  76.0  34.216873       0   154.9   82.10           306.0   \n","15  48.0  26.516476       1   180.3   86.20            96.0   \n","16  82.0  18.921389       0   154.9   45.40           164.0   \n","17  78.0  36.668167       0   167.6  103.00           282.0   \n","18  62.0  19.108088       1   157.5   47.40           275.0   \n","19  73.0  22.851562       0   160.0   58.50           178.0   \n","20  73.0  20.351971       0   167.5   57.10           159.0   \n","21  59.0  27.109238       1   177.8   85.70            85.0   \n","22  51.0  37.954367       0   172.7  113.20           168.0   \n","23  80.0  36.267895       1   180.3  117.90           138.0   \n","24  74.0  25.856081       1   170.2   74.90           145.0   \n","25  61.0  29.161972       1   180.3   94.80           267.0   \n","26  74.0  38.111136       1   188.0  134.70           279.0   \n","27  65.0  53.414791       0   166.4  147.90           121.0   \n","28  38.0  33.438787       0   161.3   87.00           135.0   \n","29  82.0  32.263238       0   162.6   85.30           111.0   \n","\n","    heart_rate_apache  resprate_apache  sodium_apache  diabetes_mellitus  \n","0               101.0             49.0          137.0                  0  \n","1                39.0             33.0          144.0                  0  \n","2               120.0             31.0          141.0                  0  \n","3                48.0             36.0          140.0                  1  \n","4               119.0              6.0          144.0                  0  \n","5                60.0             32.0          137.0                  0  \n","6                49.0             36.0          142.0                  0  \n","7                59.0              6.0          145.0                  1  \n","8                92.0             30.0          136.0                  0  \n","9               118.0             52.0          138.0                  0  \n","10              100.0             46.0          138.0                  0  \n","11               90.0             37.0          141.0                  1  \n","12              108.0             45.0          135.0                  0  \n","13               61.0             30.0          139.0                  0  \n","14              112.0             40.0          130.0                  1  \n","15              133.0             31.0          137.0                  0  \n","16              103.0             48.0          134.0                  0  \n","17              104.0             42.0          138.0                  1  \n","18              108.0             38.0          131.0                  1  \n","19              154.0             55.0          138.0                  1  \n","20              110.0             27.0          139.0                  0  \n","21               92.0              6.0          143.0                  0  \n","22               60.0             43.0          119.0                  1  \n","23              178.0             43.0          140.0                  1  \n","24               40.0             16.0          146.0                  1  \n","25               62.0             58.0          134.0                  1  \n","26              112.0             43.0          132.0                  1  \n","27               88.0             32.0          142.0                  0  \n","28               46.0             28.0          138.0                  0  \n","29              114.0             22.0          143.0                  1  "]},"execution_count":785,"metadata":{},"output_type":"execute_result"}],"source":["input_data = pd.read_csv('hw2_input_basic.csv')\n","input_data"]},{"cell_type":"markdown","metadata":{"id":"BhtqUTG9Nlyz"},"source":["## Global attributes\n","Define the global attributes\n","> Note : You **cannot** modify the values of these attributes we given in the basic part"]},{"cell_type":"code","execution_count":786,"metadata":{"id":"etfPC94oN_TO"},"outputs":[],"source":["max_depth = 2\n","depth = 0\n","min_samples_split = 2\n","n_features = input_data.shape[1] - 1"]},{"cell_type":"markdown","metadata":{"id":"V1FN1Z-tOFOo"},"source":["> You can add your own global attributes here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KQ-OYop8ONnv"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Gey7t_Yx5YML"},"source":["## Step2 : Calculate the Entropy and Information Gain \n","Calculate the information gain and entropy values before separate data into left subtree and right subtree"]},{"cell_type":"code","execution_count":787,"metadata":{"id":"hpdNz3ij6keH"},"outputs":[{"name":"stdout","output_type":"stream","text":["ans_entropy =  0.9871377743721863\n"]}],"source":["def entropy(data):\n","  \"\"\"\n","  This function measures the amount of uncertainty in a probability distribution\n","  args: \n","  * data(type: DataFrame): the data you're calculating for the entropy\n","  return:\n","  * entropy_value(type: float): the data's entropy\n","  \"\"\"\n","  data_size = len(data)\n","  \n","  # No data\n","  if data_size == 0:\n","    return 0\n","  \n","  p = data['diabetes_mellitus'].sum() / data_size\n","  \n","  # Uncertainty = 0\n","  if p == 0 or p == 1: \n","    return 0\n","  \n","  entropy_value = -(p*math.log2(p)) - (1-p)*math.log2(1-p)\n","  \n","  return entropy_value\n","\n","# [Note] You have to save the value of \"ans_entropy\" into the output file\n","ans_entropy = entropy(input_data)\n","print(\"ans_entropy = \", ans_entropy)"]},{"cell_type":"code","execution_count":788,"metadata":{"id":"zCC_SiU26kbX"},"outputs":[{"name":"stdout","output_type":"stream","text":["ans_informationGain =  0.0834598868480716\n"]}],"source":["def information_gain(data, mask):\n","  \"\"\"\n","  This function will calculate the information gain\n","  args:\n","  * data(type: DataFrame): the data you're calculating for the information gain\n","  * mask(type: Series): partition information(left/right) of current input data, \n","    - boolean 1(True) represents split to left subtree\n","    - boolean 0(False) represents split to right subtree\n","  return:\n","  * ig(type: float): the information gain you can obtain by classify data with this given mask\n","  \"\"\"\n","  data_size = len(data)\n","  \n","  left_subtree = pd.DataFrame(columns=data.columns)\n","  right_subtree = pd.DataFrame(columns=data.columns)\n","  \n","  # Partition data\n","  for index in range(data_size):\n","    if mask[index] == True:\n","      left_subtree.loc[len(left_subtree)] = data.loc[index]\n","    else:\n","      right_subtree.loc[len(right_subtree)] = data.loc[index]\n","    \n","  # Calculate entropy\n","  entropy_before = entropy(data)\n","  left_size = len(left_subtree)\n","  right_size = len(right_subtree)\n","  left_entropy = entropy(left_subtree)\n","  right_entropy = entropy(right_subtree)\n","  \n","  entropy_after = left_size/data_size*left_entropy + right_size/data_size*right_entropy\n","  \n","  ig = entropy_before - entropy_after\n","\n","  return ig\n","\n","# [Note] You have to save the value of \"ans_informationGain\" into your output file\n","temp1 = np.zeros((int(input_data.shape[0]/4), 1), dtype=bool)\n","temp2 = np.ones(((input_data.shape[0]-int(input_data.shape[0]/4), 1)), dtype=bool)\n","temp_mask = np.concatenate((temp1, temp2))\n","df_mask = pd.DataFrame(temp_mask, columns=['mask'])\n","ans_informationGain = information_gain(input_data, df_mask['mask'])\n","print(\"ans_informationGain = \", ans_informationGain)"]},{"cell_type":"markdown","metadata":{"id":"9r8mrn7A55if"},"source":["## Step3 : Find the Best Split\n","Find the best split combination, **feature** and **threshold**, by calculating the information gain\n"]},{"cell_type":"code","execution_count":789,"metadata":{"id":"D6gg7ig18XgM"},"outputs":[{"name":"stdout","output_type":"stream","text":["ans_ig =  0.3522950515812332\n","ans_value =  207.0\n","ans_name =  glucose_apache\n"]}],"source":["def find_best_split(data):\n","  \"\"\"\n","  This function will find the best split combination of data\n","  args:\n","  * data(type: DataFrame): the input data\n","  return\n","  * best_ig(type: float): the best information gain you obtain\n","  * best_threshold(type: float): the value that splits data into 2 branches\n","  * best_feature(type: string): the feature that splits data into 2 branches\n","  \"\"\"\n","  best_ig = 0\n","  best_threshold = 0\n","  best_feature = ''\n","  \n","  columns_name = list(data.columns[:-1])\n","  if n_features >= len(columns_name):\n","    random_features = columns_name\n","  else:\n","    random_features = random.sample(columns_name, n_features)\n","  \n","  for feature in random_features:\n","    possible_values = np.sort(data[feature].unique())\n","    for middle_point in possible_values:\n","      mask = data[feature] <= middle_point\n","      current_ig = information_gain(data, mask)\n","      \n","      if current_ig > best_ig:\n","        best_ig = current_ig\n","        best_threshold = middle_point\n","        best_feature = feature\n","      \n","  return best_ig, best_threshold, best_feature\n","\n","\n","# [Note] You have to save the value of \"ans_ig\", \"ans_value\", and \"ans_name\" into the output file\n","ans_ig, ans_value, ans_name = find_best_split(input_data)\n","print(\"ans_ig = \", ans_ig)\n","print(\"ans_value = \", ans_value)\n","print(\"ans_name = \", ans_name)"]},{"cell_type":"markdown","metadata":{"id":"61hPUYvy6MTB"},"source":["## Step4 : Split into 2 branches\n","Using the best split combination you find in function *find_best_split()* to split data into Left Subtree and Right Subtree "]},{"cell_type":"code","execution_count":790,"metadata":{"id":"KQRcjzCLCo4R"},"outputs":[{"name":"stdout","output_type":"stream","text":["ans_left =  10\n"]}],"source":["def make_partition(data, feature, threshold):\n","  \"\"\"\n","  This function will split the data into 2 branches\n","  args:\n","  * data(type: DataFrame): the input data\n","  * feature(type: string): the attribute(column name)\n","  * threshold(type: float): the threshold for splitting the data\n","  return:\n","  * left(type: DataFrame): the divided data that matches(less than or equal to) the assigned feature's threshold\n","  * right(type: DataFrame): the divided data that doesn't match the assigned feature's threshold\n","  \"\"\"\n","  data_size = len(data)\n","  \n","  left = pd.DataFrame(columns=data.columns)\n","  right = pd.DataFrame(columns=data.columns)\n","  \n","  mask = data[feature] <= threshold\n","  \n","  # Partition data\n","  for index in range(data_size):\n","    if mask[index] == True:\n","      left.loc[len(left)] = data.loc[index]\n","    else:\n","      right.loc[len(right)] = data.loc[index]\n","  \n","  return left, right\n","\n","\n","# [Note] You have to save the value of \"ans_left\" into the output file\n","left, right = make_partition(input_data, 'age', 61.0)\n","ans_left = left.shape[0]\n","print(\"ans_left = \", ans_left)"]},{"cell_type":"markdown","metadata":{"id":"GLzy6Yhg802x"},"source":["## Step5 : Build Decision Tree\n","Use the above functions to implement the decision tree\n","\n","Instructions: \n","1.  If current depth < max_depth and the remaining number of samples > min_samples_split: continue to classify those samples\n","2.  Use function *find_best_split()* to find the best split combination\n","3.  If the obtained information gain is **greater than 0**: can build a deeper decision tree (add depth)\n","4. Use function *make_partition()* to split the data into two parts\n","5. Save the features and corresponding thresholds (starting from the root) used by the decision tree into *ans_features[]* and *ans_thresholds[]* respectively\n","\n","\n"]},{"cell_type":"code","execution_count":791,"metadata":{"id":"_OAXVddKkvM2"},"outputs":[],"source":["def build_tree(data, max_depth, min_samples_split, depth):\n","  \"\"\"\n","  This function will build the decision tree\n","  args:\n","  * data(type: DataFrame): the data you want to apply to the decision tree\n","  * max_depth: the maximum depth of a decision tree\n","  * min_samples_split: the minimum number of instances required to do partition\n","  * depth: the height of the current decision tree\n","  return:\n","  * subtree: the decision tree structure including root, branch, and leaf (with the attributes and thresholds)\n","  \"\"\"\n","  def Majority(data):\n","    positive_case = data['diabetes_mellitus'].sum()\n","    negative_case = len(data) - positive_case\n","    \n","    if positive_case > negative_case:\n","      return 1\n","    else:\n","      return 0\n","\n","  # check the condition of current depth and the remaining number of samples\n","  if depth < max_depth and len(data) > min_samples_split:\n","    # call find_best_split() to find the best combination\n","    ig, threshold, feature = find_best_split(data)\n","    # check the value of information gain is greater than 0 or not \n","    if ig > 0:\n","      # update the depth\n","      depth = depth + 1\n","      # call make_partition() to split the data into two parts\n","      left, right = make_partition(data, feature, threshold)\n","      # If there is no data split to the left tree OR no data split to the left tree\n","      if len(left) <= 0 or len(right) <= 0:\n","        # return the label of the majority\n","        if len(left) <= 0:\n","          label = Majority(right)\n","        else:\n","          label = Majority(left)\n","        return label\n","      else:\n","        question = \"{} {} {}\".format(feature, \"<=\", threshold)\n","        subtree = {question: []}\n","\n","        # call function build_tree() to recursively build the left subtree and right subtree\n","        left_subtree = build_tree(left, max_depth, min_samples_split, depth)\n","        right_subtree = build_tree(right, max_depth, min_samples_split, depth)\n","        if left_subtree == right_subtree:\n","          subtree = left_subtree\n","        else:\n","          subtree[question].append(left_subtree)\n","          subtree[question].append(right_subtree)\n","    else:\n","      # return the label of the majority\n","      label = Majority(data)\n","      return label\n","  else:\n","    # return the label of the majority\n","    label = Majority(data)\n","    return label\n","\n","  return subtree"]},{"cell_type":"markdown","metadata":{"id":"qlIrw9Gu-M9-"},"source":["An example of the output from *build_tree()* \n","```\n","{'bmi <= 33.5': [1, {'age <= 68.5': [0, 1]}]}\n","```\n","Therefore, \n","```\n","ans_features = ['bmi', 'age']\n","ans_thresholds = [33.5, 68.5]\n","```\n","\n"]},{"cell_type":"code","execution_count":792,"metadata":{"id":"QW8wm1rD9dlS"},"outputs":[{"data":{"text/plain":["{'glucose_apache <= 207.0': [{'heart_rate_apache <= 133.0': [0, 1]}, 1]}"]},"execution_count":792,"metadata":{},"output_type":"execute_result"}],"source":["ans_features = []\n","ans_thresholds = []\n","\n","decisionTree = build_tree(input_data, max_depth, min_samples_split, depth)\n","decisionTree"]},{"cell_type":"code","execution_count":793,"metadata":{"id":"v_n0BfNSGejN"},"outputs":[{"data":{"text/plain":["['glucose_apache', 'heart_rate_apache']"]},"execution_count":793,"metadata":{},"output_type":"execute_result"}],"source":["# [Note] You have to save the features in the \"decisionTree\" structure (from root to branch and leaf) into the output file\n","def save_features(decisionTree):\n","    if type(decisionTree) == dict:\n","        for key, value in decisionTree.items():\n","            description = key.split()\n","            yield description[0]\n","            for subTree in value:\n","                yield from save_features(subTree)\n","        \n","ans_features = [i for i in save_features(decisionTree)]\n","ans_features"]},{"cell_type":"code","execution_count":794,"metadata":{"id":"D6H9zkN_GgK-"},"outputs":[{"data":{"text/plain":["[207.0, 133.0]"]},"execution_count":794,"metadata":{},"output_type":"execute_result"}],"source":["# [Note] You have to save the corresponding thresholds for the features in the \"ans_features\" list into the output file\n","def save_thresholds(decisionTree):\n","    if type(decisionTree) == dict:\n","        for key, value in decisionTree.items():\n","            description = key.split()\n","            yield float(description[2])\n","            for subTree in value:\n","                yield from save_thresholds(subTree)\n","        \n","ans_thresholds = [i for i in save_thresholds(decisionTree)]\n","ans_thresholds"]},{"cell_type":"markdown","metadata":{"id":"rP0SU7tTweOX"},"source":["## Step6 : Save answers"]},{"cell_type":"code","execution_count":795,"metadata":{"id":"sDO36kKEwh6C"},"outputs":[],"source":["basic = []\n","basic.append(ans_entropy)\n","basic.append(ans_informationGain)\n","basic.append(ans_ig)\n","basic.append(ans_value)\n","basic.append(ans_name)\n","basic.append(ans_left)\n","for i in range(len(ans_features)):\n","  basic.append(ans_features[i])\n","for m in range(len(ans_thresholds)):\n","  basic.append(ans_thresholds[m])"]},{"cell_type":"markdown","metadata":{"id":"7DotyrSZjYKi"},"source":["## Step7 : Split data\n","Split data into training set and validation set\n","> Note: We have split the data into training set and validation. You **cannot** change the distribution of the data."]},{"cell_type":"code","execution_count":796,"metadata":{"id":"WjNM-n4i5mlG"},"outputs":[{"name":"stdout","output_type":"stream","text":["(30, 10)\n","(20, 10)\n","(10, 10)\n"]}],"source":["num_train = 20\n","num_validation = 10\n","\n","training_data = input_data.iloc[:num_train]\n","validation_data = input_data.iloc[-num_validation:]\n","\n","y_train = training_data[[\"diabetes_mellitus\"]]\n","x_train = training_data.drop(['diabetes_mellitus'], axis=1)\n","y_validation = validation_data[[\"diabetes_mellitus\"]]\n","x_validation = validation_data.drop(['diabetes_mellitus'], axis=1)\n","y_validation = y_validation.values.flatten()\n","\n","print(input_data.shape)\n","print(training_data.shape)\n","print(validation_data.shape)"]},{"cell_type":"markdown","metadata":{"id":"GfKSt2gH74Uu"},"source":["## Step8 to Step10 : Make predictions with a decision tree"]},{"cell_type":"markdown","metadata":{"id":"BZqSVoJ48a3-"},"source":["Define the attributions of the decision tree\n","> You **cannot** modify the values of these attributes in this part"]},{"cell_type":"code","execution_count":797,"metadata":{"id":"vSlZ7FVB8eau"},"outputs":[],"source":["max_depth = 2\n","depth = 0\n","min_samples_split = 2\n","n_features = x_train.shape[1]"]},{"cell_type":"markdown","metadata":{"id":"FrK-YqLmLH8p"},"source":["We have finished the function '*classify_data()*' below, however, you can modify this function if you prefer completing it on your own way."]},{"cell_type":"code","execution_count":798,"metadata":{"id":"0piZ0blpFXVq"},"outputs":[],"source":["def classify_data(instance, tree):\n","  \"\"\"\n","  This function will predict/classify the input instance\n","  args:\n","  * instance: a instance(case) to be predicted\n","  return:\n","  * answer: the prediction result (the classification result)\n","  \"\"\"\n","  equation = list(tree.keys())[0] \n","  if equation.split()[1] == '<=':\n","    temp_feature = equation.split()[0]\n","    temp_threshold = equation.split()[2]\n","    if instance[temp_feature] > float(temp_threshold):\n","      answer = tree[equation][1]\n","    else:\n","      answer = tree[equation][0]\n","  else:\n","    if instance[equation.split()[0]] in (equation.split()[2]):\n","      answer = tree[equation][0]\n","    else:\n","      answer = tree[equation][1]\n","\n","  if not isinstance(answer, dict):\n","    return answer\n","  else:\n","    return classify_data(instance, answer)\n","\n","\n","def make_prediction(tree, data):\n","  \"\"\"\n","  This function will use your pre-trained decision tree to predict the labels of all instances in data\n","  args:\n","  * tree: the decision tree\n","  * data: the data to predict\n","  return:\n","  * y_prediction: the predictions\n","  \"\"\"\n","  \n","  # [Note] You can call the function classify_data() to predict the label of each instance\n","  data_size = len(data)\n","  y_prediction = []\n","  for index in range(data_size):\n","    y_prediction.append(classify_data(data.iloc[index], tree))\n","\n","  return y_prediction\n","\n","\n","def calculate_score(y_true, y_pred):\n","  \"\"\"\n","  This function will calculate the f1-score of the predictions\n","  args:\n","  * y_true: the ground truth\n","  * y_pred: the predictions\n","  return:\n","  * score: the f1-score\n","  \"\"\"\n","  TP = 0\n","  FP = 0\n","  FN = 0\n","  TN = 0\n","  \n","  for truth, pred in zip(y_true, y_pred):\n","    if truth == 1:\n","      if pred == truth:\n","        TP = TP + 1\n","      else:\n","        FN = FN + 1\n","    else:\n","      if pred == truth:\n","        TN = TN + 1\n","      else:\n","        FP = FP + 1\n","  \n","  # print(\"TP =\", TP)\n","  # print(\"FP =\", FP)\n","  # print(\"FN =\", FN)\n","  # print(\"Tn =\", TN)\n","  \n","  Precision = TP / (TP+FN)\n","  Recall = TP / (TP+FP)\n","  \n","  score = 2 * Precision * Recall / (Precision + Recall)\n","  \n","  return score"]},{"cell_type":"code","execution_count":799,"metadata":{"id":"3IEu3z3s9TDu"},"outputs":[{"name":"stdout","output_type":"stream","text":["ans_f1score =  0.6666666666666666\n"]}],"source":["decision_tree = build_tree(training_data, max_depth, min_samples_split, depth)\n","\n","y_pred = make_prediction(decision_tree, x_validation)\n","# [Note] You have to save the value of \"ans_f1score\" the your output file\n","ans_f1score = calculate_score(y_validation, y_pred)\n","print(\"ans_f1score = \", ans_f1score)"]},{"cell_type":"markdown","metadata":{"id":"IzzOKOwn-kod"},"source":["## Step11 : Write the Output File\n","Save all of your answers in a csv file, named as **hw2_basic.csv**"]},{"cell_type":"code","execution_count":800,"metadata":{"id":"p0zsaWPL2qXn"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.9871377743721863, 0.0834598868480716, 0.3522950515812332, 207.0, 'glucose_apache', 10, 'glucose_apache', 'heart_rate_apache', 207.0, 133.0, 0.6666666666666666]\n"]}],"source":["ans_path = 'hw2_basic.csv'\n","\n","# [Note] You have to save the value of \"ans_f1score\" into the output file\n","basic.append(ans_f1score)\n","print(basic)\n","\n","pd.DataFrame(basic).to_csv(ans_path, header = None, index = None)"]},{"cell_type":"markdown","metadata":{"id":"tV25IjM7_aEn"},"source":["# **Advanced Part** (35%)"]},{"cell_type":"markdown","metadata":{"id":"knH1Ih0Pha7X"},"source":["## Step1: Load the input data\n","First, load the input file **hw2_input_advanced.csv**"]},{"cell_type":"code","execution_count":801,"metadata":{"id":"FthBdLxRhi9W"},"outputs":[],"source":["advanced_data = pd.read_csv('hw2_input_advanced.csv')"]},{"cell_type":"markdown","metadata":{"id":"vqLH49oBndRh"},"source":["You can split *advanced_data* into training set and validaiton set"]},{"cell_type":"code","execution_count":802,"metadata":{"id":"9l0hLPVjncam"},"outputs":[{"name":"stdout","output_type":"stream","text":["(8379, 25)\n","(7379, 25)\n","(1000, 25)\n"]}],"source":["num_train = 7379\n","num_validation = 1000\n","\n","training_data = advanced_data.iloc[:num_train]\n","validation_data = advanced_data.iloc[-num_validation:]\n","\n","y_train = training_data[[\"diabetes_mellitus\"]]\n","x_train = training_data.drop(['diabetes_mellitus'], axis=1)\n","y_validation = validation_data[[\"diabetes_mellitus\"]]\n","x_validation = validation_data.drop(['diabetes_mellitus'], axis=1)\n","y_validation = y_validation.values.flatten()\n","\n","print(advanced_data.shape)\n","print(training_data.shape)\n","print(validation_data.shape)"]},{"cell_type":"markdown","metadata":{"id":"tFgbUY_ajVOK"},"source":["## Step2 : Load the test data\n","Load the input file **hw2_input_test.csv** to make predictions with the pre-trained random forest model"]},{"cell_type":"code","execution_count":803,"metadata":{"id":"3hW542KWNxVF"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>bmi</th>\n","      <th>gender</th>\n","      <th>height</th>\n","      <th>weight</th>\n","      <th>arf_apache</th>\n","      <th>bun_apache</th>\n","      <th>creatinine_apache</th>\n","      <th>gcs_eyes_apache</th>\n","      <th>gcs_motor_apache</th>\n","      <th>...</th>\n","      <th>hematocrit_apache</th>\n","      <th>intubated_apache</th>\n","      <th>map_apache</th>\n","      <th>resprate_apache</th>\n","      <th>sodium_apache</th>\n","      <th>temp_apache</th>\n","      <th>ventilated_apache</th>\n","      <th>wbc_apache</th>\n","      <th>apache_4a_hospital_death_prob</th>\n","      <th>apache_4a_icu_death_prob</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>62</td>\n","      <td>32.866392</td>\n","      <td>1</td>\n","      <td>177.80</td>\n","      <td>103.9</td>\n","      <td>1</td>\n","      <td>31.0</td>\n","      <td>10.30</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>...</td>\n","      <td>36.4</td>\n","      <td>0</td>\n","      <td>157</td>\n","      <td>26</td>\n","      <td>134</td>\n","      <td>36.1</td>\n","      <td>0</td>\n","      <td>4.56</td>\n","      <td>0.06</td>\n","      <td>0.03</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>82</td>\n","      <td>23.582766</td>\n","      <td>0</td>\n","      <td>157.50</td>\n","      <td>58.5</td>\n","      <td>0</td>\n","      <td>26.0</td>\n","      <td>0.54</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>...</td>\n","      <td>32.8</td>\n","      <td>0</td>\n","      <td>42</td>\n","      <td>25</td>\n","      <td>142</td>\n","      <td>36.1</td>\n","      <td>0</td>\n","      <td>6.00</td>\n","      <td>0.14</td>\n","      <td>0.06</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>61</td>\n","      <td>31.684520</td>\n","      <td>1</td>\n","      <td>172.70</td>\n","      <td>94.5</td>\n","      <td>0</td>\n","      <td>16.0</td>\n","      <td>1.11</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>...</td>\n","      <td>35.3</td>\n","      <td>0</td>\n","      <td>129</td>\n","      <td>6</td>\n","      <td>131</td>\n","      <td>36.8</td>\n","      <td>0</td>\n","      <td>8.59</td>\n","      <td>0.05</td>\n","      <td>0.03</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>58</td>\n","      <td>45.156250</td>\n","      <td>0</td>\n","      <td>160.00</td>\n","      <td>115.6</td>\n","      <td>0</td>\n","      <td>19.0</td>\n","      <td>0.70</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>...</td>\n","      <td>30.1</td>\n","      <td>1</td>\n","      <td>131</td>\n","      <td>23</td>\n","      <td>138</td>\n","      <td>34.9</td>\n","      <td>1</td>\n","      <td>16.03</td>\n","      <td>0.33</td>\n","      <td>0.22</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>74</td>\n","      <td>25.817016</td>\n","      <td>1</td>\n","      <td>172.70</td>\n","      <td>77.0</td>\n","      <td>0</td>\n","      <td>25.0</td>\n","      <td>0.93</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>...</td>\n","      <td>34.5</td>\n","      <td>0</td>\n","      <td>55</td>\n","      <td>12</td>\n","      <td>135</td>\n","      <td>36.3</td>\n","      <td>0</td>\n","      <td>45.80</td>\n","      <td>0.12</td>\n","      <td>0.05</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>835</th>\n","      <td>73</td>\n","      <td>17.943584</td>\n","      <td>0</td>\n","      <td>157.48</td>\n","      <td>44.5</td>\n","      <td>0</td>\n","      <td>12.0</td>\n","      <td>0.30</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>...</td>\n","      <td>33.8</td>\n","      <td>0</td>\n","      <td>129</td>\n","      <td>9</td>\n","      <td>144</td>\n","      <td>36.9</td>\n","      <td>0</td>\n","      <td>7.70</td>\n","      <td>0.02</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>836</th>\n","      <td>79</td>\n","      <td>29.049732</td>\n","      <td>1</td>\n","      <td>167.60</td>\n","      <td>81.6</td>\n","      <td>0</td>\n","      <td>48.0</td>\n","      <td>2.19</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>...</td>\n","      <td>42.7</td>\n","      <td>0</td>\n","      <td>163</td>\n","      <td>9</td>\n","      <td>139</td>\n","      <td>36.4</td>\n","      <td>0</td>\n","      <td>10.77</td>\n","      <td>0.06</td>\n","      <td>0.03</td>\n","    </tr>\n","    <tr>\n","      <th>837</th>\n","      <td>85</td>\n","      <td>24.627827</td>\n","      <td>0</td>\n","      <td>152.40</td>\n","      <td>57.2</td>\n","      <td>0</td>\n","      <td>11.0</td>\n","      <td>0.48</td>\n","      <td>3</td>\n","      <td>5</td>\n","      <td>...</td>\n","      <td>29.5</td>\n","      <td>0</td>\n","      <td>67</td>\n","      <td>9</td>\n","      <td>139</td>\n","      <td>36.6</td>\n","      <td>0</td>\n","      <td>7.35</td>\n","      <td>0.16</td>\n","      <td>0.05</td>\n","    </tr>\n","    <tr>\n","      <th>838</th>\n","      <td>68</td>\n","      <td>32.510940</td>\n","      <td>1</td>\n","      <td>193.00</td>\n","      <td>121.1</td>\n","      <td>0</td>\n","      <td>14.0</td>\n","      <td>0.64</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>...</td>\n","      <td>37.5</td>\n","      <td>0</td>\n","      <td>61</td>\n","      <td>10</td>\n","      <td>140</td>\n","      <td>36.9</td>\n","      <td>1</td>\n","      <td>16.02</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>839</th>\n","      <td>48</td>\n","      <td>24.106828</td>\n","      <td>0</td>\n","      <td>157.50</td>\n","      <td>59.8</td>\n","      <td>0</td>\n","      <td>7.0</td>\n","      <td>0.33</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>...</td>\n","      <td>32.3</td>\n","      <td>0</td>\n","      <td>111</td>\n","      <td>14</td>\n","      <td>139</td>\n","      <td>36.3</td>\n","      <td>0</td>\n","      <td>2.20</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>840 rows × 24 columns</p>\n","</div>"],"text/plain":["     age        bmi  gender  height  weight  arf_apache  bun_apache  \\\n","0     62  32.866392       1  177.80   103.9           1        31.0   \n","1     82  23.582766       0  157.50    58.5           0        26.0   \n","2     61  31.684520       1  172.70    94.5           0        16.0   \n","3     58  45.156250       0  160.00   115.6           0        19.0   \n","4     74  25.817016       1  172.70    77.0           0        25.0   \n","..   ...        ...     ...     ...     ...         ...         ...   \n","835   73  17.943584       0  157.48    44.5           0        12.0   \n","836   79  29.049732       1  167.60    81.6           0        48.0   \n","837   85  24.627827       0  152.40    57.2           0        11.0   \n","838   68  32.510940       1  193.00   121.1           0        14.0   \n","839   48  24.106828       0  157.50    59.8           0         7.0   \n","\n","     creatinine_apache  gcs_eyes_apache  gcs_motor_apache  ...  \\\n","0                10.30                4                 6  ...   \n","1                 0.54                3                 4  ...   \n","2                 1.11                4                 6  ...   \n","3                 0.70                1                 4  ...   \n","4                 0.93                4                 6  ...   \n","..                 ...              ...               ...  ...   \n","835               0.30                4                 6  ...   \n","836               2.19                4                 6  ...   \n","837               0.48                3                 5  ...   \n","838               0.64                4                 6  ...   \n","839               0.33                4                 6  ...   \n","\n","     hematocrit_apache  intubated_apache  map_apache  resprate_apache  \\\n","0                 36.4                 0         157               26   \n","1                 32.8                 0          42               25   \n","2                 35.3                 0         129                6   \n","3                 30.1                 1         131               23   \n","4                 34.5                 0          55               12   \n","..                 ...               ...         ...              ...   \n","835               33.8                 0         129                9   \n","836               42.7                 0         163                9   \n","837               29.5                 0          67                9   \n","838               37.5                 0          61               10   \n","839               32.3                 0         111               14   \n","\n","     sodium_apache  temp_apache  ventilated_apache  wbc_apache  \\\n","0              134         36.1                  0        4.56   \n","1              142         36.1                  0        6.00   \n","2              131         36.8                  0        8.59   \n","3              138         34.9                  1       16.03   \n","4              135         36.3                  0       45.80   \n","..             ...          ...                ...         ...   \n","835            144         36.9                  0        7.70   \n","836            139         36.4                  0       10.77   \n","837            139         36.6                  0        7.35   \n","838            140         36.9                  1       16.02   \n","839            139         36.3                  0        2.20   \n","\n","     apache_4a_hospital_death_prob  apache_4a_icu_death_prob  \n","0                             0.06                      0.03  \n","1                             0.14                      0.06  \n","2                             0.05                      0.03  \n","3                             0.33                      0.22  \n","4                             0.12                      0.05  \n","..                             ...                       ...  \n","835                           0.02                      0.01  \n","836                           0.06                      0.03  \n","837                           0.16                      0.05  \n","838                           0.00                      0.00  \n","839                           0.01                      0.00  \n","\n","[840 rows x 24 columns]"]},"execution_count":803,"metadata":{},"output_type":"execute_result"}],"source":["x_test = pd.read_csv('hw2_input_test.csv')\n","x_test"]},{"cell_type":"markdown","metadata":{"id":"mH-0DxyR9qWn"},"source":["## Step3 : Build a Random Forest"]},{"cell_type":"markdown","metadata":{"id":"8xbLxFW597FG"},"source":["Define the attributions of the random forest\n","> * You **can** modify the values of these attributes in advanced part\n","> * Each tree can have different attribute values\n","> * There must be **at least** 3 decision trees in the random forest model\n","> * Must use function *build_tree()* to build a random forest model\n","> * These are the parameters you can adjust : \n","\n","\n","    ```\n","    max_depth = \n","    depth = 0\n","    min_samples_split = \n","    \n","    # total number of trees in a random forest\n","    n_trees = \n","\n","    # number of features to train a decision tree\n","    n_features = \n","\n","    # the ratio to select the number of instances\n","    sample_size = \n","    n_samples = int(training_data.shape[0] * sample_size)\n","    ```\n","\n","\n"]},{"cell_type":"code","execution_count":804,"metadata":{"id":"LD8ndJ8ymzG3"},"outputs":[],"source":["# Define the attributes\n","max_depth = 3\n","depth = 0\n","min_samples_split = 5\n","n_trees = 17\n","n_features = 16\n","sample_size = 1/90\n","n_samples = int(training_data.shape[0] * sample_size)"]},{"cell_type":"code","execution_count":805,"metadata":{"id":"hVl66f1aU36-"},"outputs":[],"source":["def build_forest(data, n_trees, n_features, n_samples):\n","  \"\"\"\n","  This function will build a random forest. \n","  args:\n","  * data: all data that can be used to train a random forest\n","  * n_trees: total number of tree\n","  * n_features: number of features\n","  * n_samples: number of instances\n","  return:\n","  * forest: a random forest with 'n_trees' of decision tree\n","  \"\"\"\n","\n","  forest = []\n","  # must reuse function build_tree()\n","  for i in range(n_trees):\n","    random_data = data.sample(n_samples)\n","    random_data.reset_index(drop=True, inplace=True)\n","    tree = build_tree(random_data, max_depth, min_samples_split, depth)\n","    forest.append(tree)\n","\n","  return forest"]},{"cell_type":"code","execution_count":806,"metadata":{"id":"zylo6C51m3OJ"},"outputs":[],"source":["forest = build_forest(training_data, n_trees, n_features, n_samples)"]},{"cell_type":"markdown","metadata":{"id":"dZb6EEYnnO05"},"source":["## Step4 : Make predictions with the random forest\n","> Note: Please print the f1-score of the predictions of each decision tree"]},{"cell_type":"code","execution_count":807,"metadata":{"id":"UbHMZnMDnWpG"},"outputs":[{"name":"stdout","output_type":"stream","text":["F1-Score =  0.6639919759277834\n","F1-Score =  0.6044703595724005\n","F1-Score =  0.5163551401869159\n","F1-Score =  0.6262626262626263\n","F1-Score =  0.6022471910112359\n","F1-Score =  0.5707133917396746\n","F1-Score =  0.6211453744493393\n","F1-Score =  0.548148148148148\n","F1-Score =  0.6348884381338743\n","F1-Score =  0.6055045871559633\n","F1-Score =  0.5790645879732739\n","F1-Score =  0.6230200633579726\n","F1-Score =  0.4639769452449568\n","F1-Score =  0.5959367945823928\n","F1-Score =  0.670391061452514\n","F1-Score =  0.517385257301808\n","F1-Score =  0.6386740331491711\n","Final-Score =  0.6835722160970232\n"]}],"source":["def make_prediction_forest(forest, data):\n","  \"\"\"\n","  This function will use the pre-trained random forest to make the predictions\n","  args:\n","  * forest: the random forest\n","  * data: the data used to predict\n","  return:\n","  * y_prediction: the predicted results\n","  \"\"\"\n","  def Vote(tree_predictions):\n","    positive_pred = tree_predictions.count(1)\n","    negative_pred = tree_predictions.count(0)\n","    \n","    if positive_pred > negative_pred:\n","      return 1\n","    else:\n","      return 0\n","  \n","  data_size = len(data)\n","  y_prediction = []\n","  for index in range(data_size):\n","    tree_predictions = []\n","    for tree in forest:\n","      tree_predictions.append(classify_data(data.iloc[index], tree))\n","    vote_result = Vote(tree_predictions)\n","    y_prediction.append(vote_result)\n","\n","  return y_prediction\n","\n","bad_trees = []\n","for i in range(n_trees):\n","  y_pred_tree = make_prediction(forest[i], x_validation)\n","  ans_f1score = calculate_score(y_validation, y_pred_tree)\n","  print(\"F1-Score = \", ans_f1score)\n","  if ans_f1score <= 0.55:\n","    bad_trees.append(forest[i])\n","\n","for tree in bad_trees:\n","  forest.remove(tree)\n","\n","y_pred_forest = make_prediction_forest(forest, x_validation)\n","score = calculate_score(y_validation, y_pred_forest)\n","print(\"Final-Score = \", score)"]},{"cell_type":"code","execution_count":808,"metadata":{"id":"Hcd70ubwgHq4"},"outputs":[],"source":["y_pred_test = make_prediction_forest(forest, x_test)"]},{"cell_type":"markdown","metadata":{"id":"2ufa5bP9HveO"},"source":["## Step5 : Write the Output File\n","Save your predictions from the **random forest** in a csv file, named as **hw2_advanced.csv**"]},{"cell_type":"code","execution_count":809,"metadata":{"id":"XdAQcE41JJYB"},"outputs":[],"source":["advanced = []\n","for i in range(len(y_pred_test)):\n","  advanced.append(y_pred_test[i])"]},{"cell_type":"code","execution_count":810,"metadata":{"id":"Pq121klSHwWO"},"outputs":[],"source":["advanced_path = 'hw2_advanced.csv'\n","pd.DataFrame(advanced).to_csv(advanced_path, header = None, index = None)"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
