{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Mount Google Drive (optional)"],"metadata":{"id":"fI00-KuddN2u"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","# os.chdir(\"/content/drive/MyDrive/....\")  # file path\n","print(os.getcwd())"],"metadata":{"id":"Uf6ucZcj6kpK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **HW2 : Decision Tree and Random Forest**\n","In *assignment 2*, you need to finish :\n","\n","1. Basic Part : Implement a **Decision Tree** model and predict whether the patients in the validation set have diabetes\n","> * Step 1 : Load the input data\n","> * Step 2 : Calculate the Entropy and Information Gain\n","> * Step 3 : Find the Best Split\n","> * Step 4 : Split into 2 branches\n","> * Step 5 : Build decision tree\n","> * Step 6 : Save the answers from step2 to step5\n","> * Step 7 : Split data into training set and validation set\n","> * Step 8 : Train a decision tree model with training set\n","> * Step 9 : Predict the cases in the *validation set* by using the model trained in *Step8*\n","> * Step 10 : Calculate the f1-score of your predictions in *Step9*\n","> * Step 11 : Write the Output File\n","\n","2. Advanced Part : Build a **Random Forest** model to make predictions\n","> * Step 1 : Load the input data\n","> * Step 2 : Load the test data\n","> * Step 3 : Build a random forest\n","> * Step 4 : Predict the cases in the test data by using the model trained in *Step3*\n","> * Step 5 : Save the predictions(from *Step 4*) in a csv file\n","\n"],"metadata":{"id":"yvRo67Io4NKF"}},{"cell_type":"markdown","source":["# **Basic Part** (60%)\n","In this part, your need to implement a Decision Tree model by completing the following given functions.\n","\n","Also, you need to run these functions with the given input variables and save the output in a csv file **hw2_basic.csv**"],"metadata":{"id":"wwVh8lYD4kbV"}},{"cell_type":"markdown","source":["## Import Packages\n","\n","\n","> Note : You **cannot** import any other packages in both basic part and advanced part\n","\n","\n","\n","\n"],"metadata":{"id":"h2ibEyDa46X2"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import math\n","import random\n","from numpy import sqrt\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"RMjaYVZD6kmb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step1: Load the input data\n","First, load the input file **hw2_input_basic.csv**"],"metadata":{"id":"zrQXqH475G8-"}},{"cell_type":"code","source":["input_data = pd.read_csv('hw2_input_basic.csv')\n","input_data"],"metadata":{"id":"0n3gcL2l6kjb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Global attributes\n","Define the global attributes\n","> Note : You **cannot** modify the values of these attributes we given in the basic part"],"metadata":{"id":"BhtqUTG9Nlyz"}},{"cell_type":"code","source":["max_depth = 2\n","depth = 0\n","min_samples_split = 2\n","n_features = input_data.shape[1] - 1"],"metadata":{"id":"etfPC94oN_TO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> You can add your own global attributes here"],"metadata":{"id":"V1FN1Z-tOFOo"}},{"cell_type":"code","source":[],"metadata":{"id":"KQ-OYop8ONnv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step2 : Calculate the Entropy and Information Gain \n","Calculate the information gain and entropy values before separate data into left subtree and right subtree"],"metadata":{"id":"Gey7t_Yx5YML"}},{"cell_type":"code","source":["def entropy(data):\n","  \"\"\"\n","  This function measures the amount of uncertainty in a probability distribution\n","  args: \n","  * data(type: DataFrame): the data you're calculating for the entropy\n","  return:\n","  * entropy_value(type: float): the data's entropy\n","  \"\"\"\n","\n","  \n","  return entropy_value\n","\n","# [Note] You have to save the value of \"ans_entropy\" into the output file\n","ans_entropy = entropy(input_data)\n","print(\"ans_entropy = \", ans_entropy)"],"metadata":{"id":"hpdNz3ij6keH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def information_gain(data, mask):\n","  \"\"\"\n","  This function will calculate the information gain\n","  args:\n","  * data(type: DataFrame): the data you're calculating for the information gain\n","  * mask(type: Series): partition information(left/right) of current input data, \n","    - boolean 1(True) represents split to left subtree\n","    - boolean 0(False) represents split to right subtree\n","  return:\n","  * ig(type: float): the information gain you can obtain by classify data with this given mask\n","  \"\"\"\n","\n","\n","  return ig\n","\n","# [Note] You have to save the value of \"ans_informationGain\" into your output file\n","temp1 = np.zeros((int(input_data.shape[0]/4), 1), dtype=bool)\n","temp2 = np.ones(((input_data.shape[0]-int(input_data.shape[0]/4), 1)), dtype=bool)\n","temp_mask = np.concatenate((temp1, temp2))\n","df_mask = pd.DataFrame(temp_mask, columns=['mask'])\n","ans_informationGain = information_gain(input_data, df_mask['mask'])\n","print(\"ans_informationGain = \", ans_informationGain)"],"metadata":{"id":"zCC_SiU26kbX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step3 : Find the Best Split\n","Find the best split combination, **feature** and **threshold**, by calculating the information gain\n"],"metadata":{"id":"9r8mrn7A55if"}},{"cell_type":"code","source":["def find_best_split(data):\n","  \"\"\"\n","  This function will find the best split combination of data\n","  args:\n","  * data(type: DataFrame): the input data\n","  return\n","  * best_ig(type: float): the best information gain you obtain\n","  * best_threshold(type: float): the value that splits data into 2 branches\n","  * best_feature(type: string): the feature that splits data into 2 branches\n","  \"\"\"\n","\n","  \n","  return best_ig, best_threshold, best_feature\n","\n","\n","# [Note] You have to save the value of \"ans_ig\", \"ans_value\", and \"ans_name\" into the output file\n","ans_ig, ans_value, ans_name = find_best_split(input_data)\n","print(\"ans_ig = \", ans_ig)\n","print(\"ans_value = \", ans_value)\n","print(\"ans_name = \", ans_name)"],"metadata":{"id":"D6gg7ig18XgM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step4 : Split into 2 branches\n","Using the best split combination you find in function *find_best_split()* to split data into Left Subtree and Right Subtree "],"metadata":{"id":"61hPUYvy6MTB"}},{"cell_type":"code","source":["def make_partition(data, feature, threshold):\n","  \"\"\"\n","  This function will split the data into 2 branches\n","  args:\n","  * data(type: DataFrame): the input data\n","  * feature(type: string): the attribute(column name)\n","  * threshold(type: float): the threshold for splitting the data\n","  return:\n","  * left(type: DataFrame): the divided data that matches(less than or equal to) the assigned feature's threshold\n","  * right(type: DataFrame): the divided data that doesn't match the assigned feature's threshold\n","  \"\"\"\n","\n","  \n","  return left, right\n","\n","\n","# [Note] You have to save the value of \"ans_left\" into the output file\n","left, right = make_partition(input_data, 'age', 61.0)\n","ans_left = left.shape[0]\n","print(\"ans_left = \", ans_left)"],"metadata":{"id":"KQRcjzCLCo4R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step5 : Build Decision Tree\n","Use the above functions to implement the decision tree\n","\n","Instructions: \n","1.  If current depth < max_depth and the remaining number of samples > min_samples_split: continue to classify those samples\n","2.  Use function *find_best_split()* to find the best split combination\n","3.  If the obtained information gain is **greater than 0**: can build a deeper decision tree (add depth)\n","4. Use function *make_partition()* to split the data into two parts\n","5. Save the features and corresponding thresholds (starting from the root) used by the decision tree into *ans_features[]* and *ans_thresholds[]* respectively\n","\n","\n"],"metadata":{"id":"GLzy6Yhg802x"}},{"cell_type":"code","source":["def build_tree(data, max_depth, min_samples_split, depth):\n","  \"\"\"\n","  This function will build the decision tree\n","  args:\n","  * data(type: DataFrame): the data you want to apply to the decision tree\n","  * max_depth: the maximum depth of a decision tree\n","  * min_samples_split: the minimum number of instances required to do partition\n","  * depth: the height of the current decision tree\n","  return:\n","  * subtree: the decision tree structure including root, branch, and leaf (with the attributes and thresholds)\n","  \"\"\"\n","\n","  # check the condition of current depth and the remaining number of samples\n","  if ..... :\n","    # call find_best_split() to find the best combination\n","\n","    # check the value of information gain is greater than 0 or not \n","    if ..... :\n","      # update the depth\n","      \n","      # call make_partition() to split the data into two parts\n","\n","      # If there is no data split to the left tree OR no data split to the left tree\n","      if ..... :\n","        # return the label of the majority\n","        label = .....\n","        return label\n","      else:\n","        question = \"{} {} {}\".format(feature, \"<=\", threshold)\n","        subtree = {question: []}\n","\n","        # call function build_tree() to recursively build the left subtree and right subtree\n","\n","        if left_subtree == right_subtree:\n","          subtree = left_subtree\n","        else:\n","          subtree[question].append(left_subtree)\n","          subtree[question].append(right_subtree)\n","    else:\n","      # return the label of the majority\n","      label = .....\n","      return label\n","  else:\n","    # return the label of the majority\n","    label = .....\n","    return label\n","\n","  return subtree"],"metadata":{"id":"_OAXVddKkvM2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["An example of the output from *build_tree()* \n","```\n","{'bmi <= 33.5': [1, {'age <= 68.5': [0, 1]}]}\n","```\n","Therefore, \n","```\n","ans_features = ['bmi', 'age']\n","ans_thresholds = [33.5, 68.5]\n","```\n","\n"],"metadata":{"id":"qlIrw9Gu-M9-"}},{"cell_type":"code","source":["ans_features = []\n","ans_thresholds = []\n","\n","decisionTree = build_tree(input_data, max_depth, min_samples_split, depth)\n","decisionTree"],"metadata":{"id":"QW8wm1rD9dlS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# [Note] You have to save the features in the \"decisionTree\" structure (from root to branch and leaf) into the output file\n","ans_features"],"metadata":{"id":"v_n0BfNSGejN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# [Note] You have to save the corresponding thresholds for the features in the \"ans_features\" list into the output file\n","ans_thresholds"],"metadata":{"id":"D6H9zkN_GgK-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step6 : Save answers"],"metadata":{"id":"rP0SU7tTweOX"}},{"cell_type":"code","source":["basic = []\n","basic.append(ans_entropy)\n","basic.append(ans_informationGain)\n","basic.append(ans_ig)\n","basic.append(ans_value)\n","basic.append(ans_name)\n","basic.append(ans_left)\n","for i in range(len(ans_features)):\n","  basic.append(ans_features[i])\n","for m in range(len(ans_thresholds)):\n","  basic.append(ans_thresholds[m])"],"metadata":{"id":"sDO36kKEwh6C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step7 : Split data\n","Split data into training set and validation set\n","> Note: We have split the data into training set and validation. You **cannot** change the distribution of the data."],"metadata":{"id":"7DotyrSZjYKi"}},{"cell_type":"code","source":["num_train = 20\n","num_validation = 10\n","\n","training_data = input_data.iloc[:num_train]\n","validation_data = input_data.iloc[-num_validation:]\n","\n","y_train = training_data[[\"diabetes_mellitus\"]]\n","x_train = training_data.drop(['diabetes_mellitus'], axis=1)\n","y_validation = validation_data[[\"diabetes_mellitus\"]]\n","x_validation = validation_data.drop(['diabetes_mellitus'], axis=1)\n","y_validation = y_validation.values.flatten()\n","\n","print(input_data.shape)\n","print(training_data.shape)\n","print(validation_data.shape)"],"metadata":{"id":"WjNM-n4i5mlG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step8 to Step10 : Make predictions with a decision tree"],"metadata":{"id":"GfKSt2gH74Uu"}},{"cell_type":"markdown","source":["Define the attributions of the decision tree\n","> You **cannot** modify the values of these attributes in this part"],"metadata":{"id":"BZqSVoJ48a3-"}},{"cell_type":"code","source":["max_depth = 2\n","depth = 0\n","min_samples_split = 2\n","n_features = x_train.shape[1]"],"metadata":{"id":"vSlZ7FVB8eau"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We have finished the function '*classify_data()*' below, however, you can modify this function if you prefer completing it on your own way."],"metadata":{"id":"FrK-YqLmLH8p"}},{"cell_type":"code","source":["def classify_data(instance, tree):\n","  \"\"\"\n","  This function will predict/classify the input instance\n","  args:\n","  * instance: a instance(case) to be predicted\n","  return:\n","  * answer: the prediction result (the classification result)\n","  \"\"\"\n","  equation = list(tree.keys())[0] \n","  if equation.split()[1] == '<=':\n","    temp_feature = equation.split()[0]\n","    temp_threshold = equation.split()[2]\n","    if instance[temp_feature] > float(temp_threshold):\n","      answer = tree[equation][1]\n","    else:\n","      answer = tree[equation][0]\n","  else:\n","    if instance[equation.split()[0]] in (equation.split()[2]):\n","      answer = tree[equation][0]\n","    else:\n","      answer = tree[equation][1]\n","\n","  if not isinstance(answer, dict):\n","    return answer\n","  else:\n","    return classify_data(instance, answer)\n","\n","\n","def make_prediction(tree, data):\n","  \"\"\"\n","  This function will use your pre-trained decision tree to predict the labels of all instances in data\n","  args:\n","  * tree: the decision tree\n","  * data: the data to predict\n","  return:\n","  * y_prediction: the predictions\n","  \"\"\"\n","  \n","  # [Note] You can call the function classify_data() to predict the label of each instance\n","\n","\n","  return y_prediction\n","\n","\n","def calculate_score(y_true, y_pred):\n","  \"\"\"\n","  This function will calculate the f1-score of the predictions\n","  args:\n","  * y_true: the ground truth\n","  * y_pred: the predictions\n","  return:\n","  * score: the f1-score\n","  \"\"\"\n","\n","  \n","  return score"],"metadata":{"id":"0piZ0blpFXVq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["decision_tree = build_tree(training_data, max_depth, min_samples_split, depth)\n","\n","y_pred = make_prediction(decision_tree, x_validation)\n","\n","# [Note] You have to save the value of \"ans_f1score\" the your output file\n","ans_f1score = calculate_score(y_validation, y_pred)\n","print(\"ans_f1score = \", ans_f1score)"],"metadata":{"id":"3IEu3z3s9TDu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step11 : Write the Output File\n","Save all of your answers in a csv file, named as **hw2_basic.csv**"],"metadata":{"id":"IzzOKOwn-kod"}},{"cell_type":"code","source":["ans_path = 'hw2_basic.csv'\n","\n","# [Note] You have to save the value of \"ans_f1score\" into the output file\n","basic.append(ans_f1score)\n","print(basic)\n","\n","pd.DataFrame(basic).to_csv(ans_path, header = None, index = None)"],"metadata":{"id":"p0zsaWPL2qXn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Advanced Part** (35%)"],"metadata":{"id":"tV25IjM7_aEn"}},{"cell_type":"markdown","source":["## Step1: Load the input data\n","First, load the input file **hw2_input_advanced.csv**"],"metadata":{"id":"knH1Ih0Pha7X"}},{"cell_type":"code","source":["advanced_data = pd.read_csv('hw2_input_advanced.csv')"],"metadata":{"id":"FthBdLxRhi9W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can split *advanced_data* into training set and validaiton set"],"metadata":{"id":"vqLH49oBndRh"}},{"cell_type":"code","source":["training_data = \n","validation_data = "],"metadata":{"id":"9l0hLPVjncam"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step2 : Load the test data\n","Load the input file **hw2_input_test.csv** to make predictions with the pre-trained random forest model"],"metadata":{"id":"tFgbUY_ajVOK"}},{"cell_type":"code","source":["x_test = pd.read_csv('hw2_input_test.csv')\n","x_test"],"metadata":{"id":"3hW542KWNxVF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step3 : Build a Random Forest"],"metadata":{"id":"mH-0DxyR9qWn"}},{"cell_type":"markdown","source":["Define the attributions of the random forest\n","> * You **can** modify the values of these attributes in advanced part\n","> * Each tree can have different attribute values\n","> * There must be **at least** 3 decision trees in the random forest model\n","> * Must use function *build_tree()* to build a random forest model\n","> * These are the parameters you can adjust : \n","\n","\n","    ```\n","    max_depth = \n","    depth = 0\n","    min_samples_split = \n","    \n","    # total number of trees in a random forest\n","    n_trees = \n","\n","    # number of features to train a decision tree\n","    n_features = \n","\n","    # the ratio to select the number of instances\n","    sample_size = \n","    n_samples = int(training_data.shape[0] * sample_size)\n","    ```\n","\n","\n"],"metadata":{"id":"8xbLxFW597FG"}},{"cell_type":"code","source":["# Define the attributes\n","\n"],"metadata":{"id":"LD8ndJ8ymzG3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_forest(data, n_trees, n_features, n_samples):\n","  \"\"\"\n","  This function will build a random forest.\n","  args:\n","  * data: all data that can be used to train a random forest\n","  * n_trees: total number of tree\n","  * n_features: number of features\n","  * n_samples: number of instances\n","  return:\n","  * forest: a random forest with 'n_trees' of decision tree\n","  \"\"\"\n","\n","  # must reuse function build_tree()\n","  tree = build_tree(.....)\n","\n","  return forest"],"metadata":{"id":"hVl66f1aU36-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["forest = build_forest(training_data, n_trees, n_features, n_samples)"],"metadata":{"id":"zylo6C51m3OJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step4 : Make predictions with the random forest\n","> Note: Please print the f1-score of the predictions of each decision tree"],"metadata":{"id":"dZb6EEYnnO05"}},{"cell_type":"code","source":["def make_prediction_forest(forest, data):\n","  \"\"\"\n","  This function will use the pre-trained random forest to make the predictions\n","  args:\n","  * forest: the random forest\n","  * data: the data used to predict\n","  return:\n","  * y_prediction: the predicted results\n","  \"\"\"\n","\n","\n","  return y_prediction"],"metadata":{"id":"UbHMZnMDnWpG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred_test = make_prediction_forest(forest, x_test)"],"metadata":{"id":"Hcd70ubwgHq4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step5 : Write the Output File\n","Save your predictions from the **random forest** in a csv file, named as **hw2_advanced.csv**"],"metadata":{"id":"2ufa5bP9HveO"}},{"cell_type":"code","source":["advanced = []\n","for i in range(len(y_pred_test)):\n","  advanced.append(y_pred_test[i])"],"metadata":{"id":"XdAQcE41JJYB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["advanced_path = 'hw2_advanced.csv'\n","pd.DataFrame(advanced).to_csv(advanced_path, header = None, index = None)"],"metadata":{"id":"Pq121klSHwWO"},"execution_count":null,"outputs":[]}]}