{"cells":[{"cell_type":"markdown","metadata":{"id":"X_Te27fi-0pP"},"source":["# **HW1: Regression** \n","In *assignment 1*, you need to finish:\n","\n","1.  Basic Part: Implement the regression model to predict the number of dengue cases\n","\n","\n","> *   Step 1: Split Data\n","> *   Step 2: Preprocess Data\n","> *   Step 3: Implement Regression\n","> *   Step 4: Make Prediction\n","> *   Step 5: Train Model and Generate Result\n","\n","2.  Advanced Part: Implementing a regression model to predict the number of dengue cases in a different way than the basic part"]},{"cell_type":"markdown","metadata":{"id":"_wDdnos-4uUv"},"source":["# 1. Basic Part (60%)\n","In the first part, you need to implement the regression to predict the number of dengue cases\n","\n","Please save the prediction result in a csv file **hw1_basic.csv**\n"]},{"cell_type":"markdown","metadata":{"id":"RzCR7vk9BFkf"},"source":["## Import Packages\n","\n","> Note: You **cannot** import any other package in the basic part"]},{"cell_type":"code","execution_count":749,"metadata":{"id":"HL5XjqFf4wSj"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import csv\n","import math\n","import random"]},{"cell_type":"markdown","metadata":{"id":"jnWjrzi0dMPz"},"source":["## Global attributes\n","Define the global attributes"]},{"cell_type":"code","execution_count":750,"metadata":{"id":"EWLDPOlHBbcK"},"outputs":[],"source":["input_dataroot = 'hw1_basic_input.csv' # Input file named as 'hw1_basic_input.csv'\n","output_dataroot = 'hw1_basic.csv' # Output file will be named as 'hw1_basic.csv'\n","\n","input_datalist =  [] # Initial datalist, saved as numpy array\n","output_datalist =  [] # Your prediction, should be 10 * 4 matrix and saved as numpy array\n","             # The format of each row should be ['epiweek', 'CityA', 'CityB', 'CityC']"]},{"cell_type":"markdown","metadata":{"id":"PsFC-cvqIcYK"},"source":["You can add your own global attributes here\n"]},{"cell_type":"code","execution_count":751,"metadata":{"id":"OUbS2BEgcut6"},"outputs":[],"source":["training_size = 94\n","testing_size = 10\n","order_temp = 1\n","order_case = 4"]},{"cell_type":"markdown","metadata":{"id":"rUoRFoQjBW5S"},"source":["## Load the Input File\n","First, load the basic input file **hw1_basic_input.csv**\n","\n","Input data would be stored in *input_datalist*"]},{"cell_type":"code","execution_count":752,"metadata":{"id":"dekR1KnqBtI6"},"outputs":[],"source":["# Read input csv to datalist\n","with open(input_dataroot, newline='') as csvfile:\n","  input_datalist = np.array(list(csv.reader(csvfile)))"]},{"cell_type":"markdown","metadata":{"id":"6kYPuikLCFx4"},"source":["## Implement the Regression Model\n","\n","> Note: It is recommended to use the functions we defined, you can also define your own functions\n"]},{"cell_type":"markdown","metadata":{"id":"jWwdx06JNEYs"},"source":["### Step 1: Split Data\n","Split data in *input_datalist* into training dataset and validation dataset \n","\n"]},{"cell_type":"code","execution_count":753,"metadata":{"id":"USDciENcB-5F"},"outputs":[],"source":["def SplitData(datalist, start, end, columns_name):\n","    dataset = pd.DataFrame(datalist[start:end], columns = columns_name)\n","    return dataset\n","\n","columns_name = ['epiweek', 'tempA', 'tempB', 'tempC', 'caseA', 'caseB', 'caseC']\n","training_dataset = SplitData(input_datalist, 1, training_size+1, columns_name)\n","testing_dataset = SplitData(input_datalist, training_size+1-order_case, training_size+1+testing_size, columns_name)"]},{"cell_type":"markdown","metadata":{"id":"u-3Qln4aNgVy"},"source":["### Step 2: Preprocess Data\n","Handle the unreasonable data\n","> Hint: Outlier and missing data can be handled by removing the data or adding the values with the help of statistics  "]},{"cell_type":"code","execution_count":754,"metadata":{"id":"XXvW1n_5NkQ5"},"outputs":[],"source":["def Normalize(datas):\n","    return datas.map(lambda data: (data-datas.min())/(datas.max()-datas.min()))\n","def PreprocessData(dataset):\n","    # Missing data\n","    missing = (dataset['tempA'] != '') & (dataset['tempB'] != '') & (dataset['tempC'] != '')\n","    dataset.where(missing, other=np.nan, inplace=True)\n","    dataset.dropna(axis='index', inplace=True)\n","\n","    # String to number\n","    for column in dataset:\n","        if column != 'epiweek':\n","            dataset[column] = dataset[column].map(float)\n","\n","    # Outlier for temp\n","    min_temp_A = dataset['tempA'].mean() - 3*dataset['tempA'].std()\n","    max_temp_A = dataset['tempA'].mean() + 3*dataset['tempA'].std()\n","    min_temp_B = dataset['tempB'].mean() - 3*dataset['tempB'].std()\n","    max_temp_B = dataset['tempB'].mean() + 3*dataset['tempB'].std()\n","    min_temp_C = dataset['tempC'].mean() - 3*dataset['tempC'].std()\n","    max_temp_C = dataset['tempC'].mean() + 3*dataset['tempC'].std()\n","    outlierA_temp = (dataset['tempA'] >= min_temp_A) & (dataset['tempA'] <= max_temp_A)\n","    outlierB_temp = (dataset['tempB'] >= min_temp_B) & (dataset['tempB'] <= max_temp_B)\n","    outlierC_temp = (dataset['tempC'] >= min_temp_C) & (dataset['tempC'] <= max_temp_C)\n","    outlier_temp = outlierA_temp & outlierB_temp & outlierC_temp\n","    dataset.where(outlier_temp, other=np.nan, inplace=True)\n","    \n","    # drop outlier\n","    dataset.dropna(axis='index', inplace=True)\n","    dataset.reset_index(drop=True, inplace=True)\n","    \n","    # Normalize\n","    dataset['tempA'] = Normalize(dataset['tempA'])\n","    dataset['tempB'] = Normalize(dataset['tempB'])\n","    dataset['tempC'] = Normalize(dataset['tempC'])\n","    \n","PreprocessData(training_dataset)\n","PreprocessData(testing_dataset)"]},{"cell_type":"markdown","metadata":{"id":"yDLpJmQUN3V6"},"source":["### Step 3: Implement Regression\n","> Hint: You can use Matrix Inversion, or Gradient Descent to finish this part\n","\n","\n"]},{"cell_type":"code","execution_count":755,"metadata":{"id":"Tx9n1_23N8C0"},"outputs":[],"source":["def Regression(x, y, learning_rate, learninig_iter, dim, N):\n","    # weights\n","    w = np.zeros((3, dim))\n","    \n","    # iterate 3 cities\n","    for city in range(3):       \n","        for i in range(learning_iter):\n","            x_trans = x[city].transpose()\n","            \n","            prediction = np.dot(x[city], w[city])\n","            loss = prediction - y[city]\n","            \n","            curr_gradient = np.dot(x_trans, loss) / (N-order_case)\n","            w[city] = w[city] - learning_rate[city] * curr_gradient\n","    \n","    return w\n","    \n","# hyperparameter\n","w_dim = 1+order_temp+order_case\n","learning_rate = [0.00001, 0.00001, 0.000001]\n","learning_iter = 150000\n","\n","# size of training_dataset\n","training_N = len(training_dataset)\n","\n","# x for training_data\n","x_temp_train = np.array([\n","    [[(training_dataset['tempA'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, training_N)],        \n","    [[(training_dataset['tempB'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, training_N)],        \n","    [[(training_dataset['tempC'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, training_N)]      \n","])\n","x_case_train = np.array([\n","    [[(training_dataset['caseA'][i-j-1]) for j in range(order_case)] for i in range(order_case, training_N)],\n","    [[(training_dataset['caseB'][i-j-1]) for j in range(order_case)] for i in range(order_case, training_N)],\n","    [[(training_dataset['caseC'][i-j-1]) for j in range(order_case)] for i in range(order_case, training_N)]\n","])\n","x_train = np.array([\n","    np.array([np.concatenate((x_temp_train[0][i], x_case_train[0][i])) for i in range(training_N-order_case)]),\n","    np.array([np.concatenate((x_temp_train[1][i], x_case_train[1][i])) for i in range(training_N-order_case)]),\n","    np.array([np.concatenate((x_temp_train[2][i], x_case_train[2][i])) for i in range(training_N-order_case)])\n","])\n","\n","# y for training_data\n","y_train = np.array([\n","    [training_dataset['caseA'][i] for i in range(order_case, training_N)],\n","    [training_dataset['caseB'][i] for i in range(order_case, training_N)],\n","    [training_dataset['caseC'][i] for i in range(order_case, training_N)]\n","])\n","\n","w = Regression(x_train, y_train, learning_rate, learning_iter, w_dim, training_N)"]},{"cell_type":"markdown","metadata":{"id":"2NxRNFwyN8xd"},"source":["### Step 4: Make Prediction\n","Make prediction of testing dataset and store the value in *output_datalist*"]},{"cell_type":"code","execution_count":756,"metadata":{"id":"EKlDIC2-N_lk"},"outputs":[],"source":["def MakePrediction(dataset, x, w, dim):\n","    # MAPE\n","    def MAPE(actual, predict):\n","        return np.mean(np.abs((actual - predict) / actual)) * 100\n","        \n","    # Predict\n","    predict_A = [] \n","    predict_B = [] \n","    predict_C = [] \n","    output_datalist = []\n","    \n","    N = len(dataset)\n","\n","    # x_case for testing_data\n","    x_caseA = [[dataset['caseA'][order_case-j-1] for j in range(order_case)]]\n","    x_caseB = [[dataset['caseB'][order_case-j-1] for j in range(order_case)]]\n","    x_caseC = [[dataset['caseC'][order_case-j-1] for j in range(order_case)]]\n","    \n","    # predict for testing_data\n","    for index in range(N-order_case):\n","        x_A = np.concatenate((x[0][index], x_caseA[index]))\n","        x_B = np.concatenate((x[1][index], x_caseB[index]))\n","        x_C = np.concatenate((x[2][index], x_caseC[index]))\n","        answer_A = sum([w[0][i] * x_A[i] for i in range(dim)])\n","        answer_B = sum([w[1][i] * x_B[i] for i in range(dim)])\n","        answer_C = sum([w[2][i] * x_C[i] for i in range(dim)])\n","        \n","        predict_A.append(round(answer_A))\n","        predict_B.append(round(answer_B))\n","        predict_C.append(round(answer_C))\n","        x_caseA.append([answer_A] + x_caseA[-1][0:-1])\n","        x_caseB.append([answer_B] + x_caseB[-1][0:-1])\n","        x_caseC.append([answer_C] + x_caseC[-1][0:-1])\n","        \n","        output_datalist.append([dataset['epiweek'][index+order_case], round(answer_A), round(answer_B), round(answer_C)])\n","        \n","    # MAPE\n","    # actual_A = dataset['caseA'][order_case:].to_numpy()\n","    # actual_B = dataset['caseB'][order_case:].to_numpy()\n","    # actual_C = dataset['caseC'][order_case:].to_numpy()\n","    \n","    # print(\"MAPE_A =\", MAPE(actual_A, predict_A))\n","    # print(\"MAPE_B =\", MAPE(actual_B, predict_B))\n","    # print(\"MAPE_C =\", MAPE(actual_C, predict_C))\n","\n","    return output_datalist\n","\n","# size of testing_dataset\n","testing_N = len(testing_dataset)\n","# x_temp for testing_data\n","x_temp_test = np.array([\n","    [[(testing_dataset['tempA'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, testing_N)],        \n","    [[(testing_dataset['tempB'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, testing_N)],        \n","    [[(testing_dataset['tempC'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, testing_N)],         \n","])\n","output_datalist = MakePrediction(testing_dataset, x_temp_test, w, w_dim)"]},{"cell_type":"markdown","metadata":{"id":"cCd0Z6izOCwq"},"source":["### Step 5: Train Model and Generate Result\n","\n","> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n","* If your regression model is *3x^2 + 2x^1 + 1*, your output would be: \n","```\n","3 2 1\n","```\n","\n","\n","\n"]},{"cell_type":"code","execution_count":757,"metadata":{"id":"iCL92EPKOFIn"},"outputs":[{"name":"stdout","output_type":"stream","text":["1.2931560984319803 2.136323239034926 0.656941346153915 0.2452831218335672 -0.08818116970644299 0.06007123016424466\n","0.5056667624358357 0.9245519373579979 0.37514942457578326 0.2368765800627512 0.07916403866494195 0.21082956307388598\n","0.11431316477322064 0.15777993102118826 0.947419358361804 0.028222972484821255 0.07545927296721004 -0.08118063278530917\n"]}],"source":["print(f\"{w[0][0]} {w[0][1]} {w[0][2]} {w[0][3]} {w[0][4]} {w[0][5]}\")\n","print(f\"{w[1][0]} {w[1][1]} {w[1][2]} {w[1][3]} {w[1][4]} {w[1][5]}\")\n","print(f\"{w[2][0]} {w[2][1]} {w[2][2]} {w[2][3]} {w[2][4]} {w[2][5]}\")"]},{"cell_type":"markdown","metadata":{"id":"J8Jhd8wAOk3D"},"source":["## Write the Output File\n","Write the prediction to output csv\n","> Format: 'epiweek', 'CityA', 'CityB', 'CityC'"]},{"cell_type":"code","execution_count":758,"metadata":{"id":"tYQVYLlKOtDB"},"outputs":[],"source":["with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n","  writer = csv.writer(csvfile)\n","  for row in output_datalist:\n","    writer.writerow(row)"]},{"cell_type":"markdown","metadata":{"id":"rx4408qg4xMQ"},"source":["# 2. Advanced Part (35%)\n","In the second part, you need to implement the regression in a different way than the basic part to help your predictions for the number of dengue cases\n","\n","We provide you with two files **hw1_advanced_input1.csv** and **hw1_advanced_input2.csv** that can help you in this part\n","\n","Please save the prediction result in a csv file **hw1_advanced.csv** \n"]},{"cell_type":"code","execution_count":759,"metadata":{"id":"DaZCe19m41g1"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.5979616029824036 1.078235379718589 0.6890531959775202 0.59755312893875 0.27090808620770446 -0.07533558108391886 0.050435713855746085\n","0.4178186412195893 0.8032221356409875 0.12320473457181418 0.3638864061211623 0.22939690075731764 0.07918171402135729 0.21552439665528714\n","0.010843452466361784 0.017045476193808723 0.20305695040751298 0.6763207581662259 0.2271679897798216 0.07982441667201459 -0.06518298232441437\n"]}],"source":["advanced_input1_dataroot = 'hw1_advanced_input1.csv'\n","advanced_output_dataroot = 'hw1_advanced.csv'\n","\n","advanced_input1_datalist = []\n","advanced_output_datalist = []\n","\n","# Read input csv to datalist\n","with open(advanced_input1_dataroot, newline='') as csvfile:\n","    advanced_input1_datalist = np.array(list(csv.reader(csvfile)))\n","    \n","# Merge basic_input and advanced_input1\n","advanced_datalist = np.concatenate((input_datalist, advanced_input1_datalist), axis=1)\n","advanced_columns_name = ['epiweek', 'tempA', 'tempB', 'tempC', 'caseA', 'caseB', 'caseC', 'epiweek2', 'precipA', 'precipB', 'precipC']\n","advanced_training_dataset = SplitData(advanced_datalist, 1, training_size+1, advanced_columns_name)\n","advanced_testing_dataset = SplitData(advanced_datalist, training_size+1-order_case, training_size+1+testing_size, advanced_columns_name)\n","advanced_training_dataset.drop(columns='epiweek2', inplace=True)\n","advanced_testing_dataset.drop(columns='epiweek2', inplace=True)\n","\n","# Preproces dataset\n","PreprocessData(advanced_training_dataset)\n","PreprocessData(advanced_testing_dataset)\n","\n","# =================================================================================================\n","\n","advanced_w_dim = 1+order_temp+1+order_case\n","# hyperparameter\n","learning_rate = [0.000007, 0.00001, 0.0000001]\n","learning_iter = 150000\n","\n","# size of training_dataset\n","advanced_training_N = len(advanced_training_dataset)\n","\n","# x for training_data\n","advanced_x_temp_train = np.array([\n","    [[(advanced_training_dataset['tempA'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, advanced_training_N)],        \n","    [[(advanced_training_dataset['tempB'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, advanced_training_N)],        \n","    [[(advanced_training_dataset['tempC'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, advanced_training_N)]      \n","])\n","advanced_x_precip_train = np.array([\n","    [advanced_training_dataset['precipA'][i] for i in range(order_case, advanced_training_N)],        \n","    [advanced_training_dataset['precipB'][i] for i in range(order_case, advanced_training_N)],        \n","    [advanced_training_dataset['precipC'][i] for i in range(order_case, advanced_training_N)]\n","])\n","advanced_x_case_train = np.array([\n","    [[(advanced_training_dataset['caseA'][i-j-1]) for j in range(order_case)] for i in range(order_case, advanced_training_N)],\n","    [[(advanced_training_dataset['caseB'][i-j-1]) for j in range(order_case)] for i in range(order_case, advanced_training_N)],\n","    [[(advanced_training_dataset['caseC'][i-j-1]) for j in range(order_case)] for i in range(order_case, advanced_training_N)]\n","])\n","advanced_x_train = np.array([\n","    np.array([np.concatenate((advanced_x_temp_train[0][i], [advanced_x_precip_train[0][i]], advanced_x_case_train[0][i])) for i in range(advanced_training_N-order_case)]),\n","    np.array([np.concatenate((advanced_x_temp_train[1][i], [advanced_x_precip_train[1][i]], advanced_x_case_train[1][i])) for i in range(advanced_training_N-order_case)]),\n","    np.array([np.concatenate((advanced_x_temp_train[2][i], [advanced_x_precip_train[2][i]], advanced_x_case_train[2][i])) for i in range(advanced_training_N-order_case)])\n","])\n","\n","# y for training_data\n","advanced_y_train = np.array([\n","    [advanced_training_dataset['caseA'][i] for i in range(order_case, advanced_training_N)],\n","    [advanced_training_dataset['caseB'][i] for i in range(order_case, advanced_training_N)],\n","    [advanced_training_dataset['caseC'][i] for i in range(order_case, advanced_training_N)]\n","])\n","\n","# w\n","advanced_w = Regression(advanced_x_train, advanced_y_train, learning_rate, learning_iter, advanced_w_dim, advanced_training_N)\n","\n","# =================================================================================================\n","\n","# size of testing_dataset\n","advanced_testing_N = len(advanced_testing_dataset)\n","\n","# x for testing_data\n","advanced_x_temp_test = np.array([\n","    [[(advanced_testing_dataset['tempA'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, advanced_testing_N)],        \n","    [[(advanced_testing_dataset['tempB'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, advanced_testing_N)],        \n","    [[(advanced_testing_dataset['tempC'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, advanced_testing_N)]      \n","])\n","advanced_x_precip_test = np.array([\n","    [advanced_testing_dataset['precipA'][i] for i in range(order_case, advanced_testing_N)],        \n","    [advanced_testing_dataset['precipB'][i] for i in range(order_case, advanced_testing_N)],        \n","    [advanced_testing_dataset['precipC'][i] for i in range(order_case, advanced_testing_N)]\n","])\n","advanced_x_test = np.array([\n","    np.array([np.concatenate((advanced_x_temp_test[0][i], [advanced_x_precip_test[0][i]])) for i in range(advanced_testing_N-order_case)]),\n","    np.array([np.concatenate((advanced_x_temp_test[1][i], [advanced_x_precip_test[1][i]])) for i in range(advanced_testing_N-order_case)]),\n","    np.array([np.concatenate((advanced_x_temp_test[2][i], [advanced_x_precip_test[2][i]])) for i in range(advanced_testing_N-order_case)])\n","])\n","\n","# Prediction\n","advanced_output_datalist = MakePrediction(advanced_testing_dataset, advanced_x_test, advanced_w, advanced_w_dim)\n","\n","print(f\"{advanced_w[0][0]} {advanced_w[0][1]} {advanced_w[0][2]} {advanced_w[0][3]} {advanced_w[0][4]} {advanced_w[0][5]} {advanced_w[0][6]}\")\n","print(f\"{advanced_w[1][0]} {advanced_w[1][1]} {advanced_w[1][2]} {advanced_w[1][3]} {advanced_w[1][4]} {advanced_w[1][5]} {advanced_w[1][6]}\")\n","print(f\"{advanced_w[2][0]} {advanced_w[2][1]} {advanced_w[2][2]} {advanced_w[2][3]} {advanced_w[2][4]} {advanced_w[2][5]} {advanced_w[2][6]}\")\n","\n","# =================================================================================================\n","\n","# Write the output\n","with open(advanced_output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n","  writer = csv.writer(csvfile)\n","  for row in advanced_output_datalist:\n","    writer.writerow(row)"]},{"cell_type":"markdown","metadata":{"id":"EtgCJU7FPeJL"},"source":["# Report *(5%)*\n","\n","Report should be submitted as a pdf file **hw1_report.pdf**\n","\n","*   Briefly describe the difficulty you encountered \n","*   Summarize your work and your reflections \n","*   No more than one page\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hlEE53_MPf4W"},"source":["# Save the Code File\n","Please save your code and submit it as an ipynb file! (**hw1.ipynb**)"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
