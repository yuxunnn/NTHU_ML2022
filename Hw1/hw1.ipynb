{"cells":[{"cell_type":"markdown","metadata":{"id":"X_Te27fi-0pP"},"source":["# **HW1: Regression** \n","In *assignment 1*, you need to finish:\n","\n","1.  Basic Part: Implement the regression model to predict the number of dengue cases\n","\n","\n","> *   Step 1: Split Data\n","> *   Step 2: Preprocess Data\n","> *   Step 3: Implement Regression\n","> *   Step 4: Make Prediction\n","> *   Step 5: Train Model and Generate Result\n","\n","2.  Advanced Part: Implementing a regression model to predict the number of dengue cases in a different way than the basic part"]},{"cell_type":"markdown","metadata":{"id":"_wDdnos-4uUv"},"source":["# 1. Basic Part (60%)\n","In the first part, you need to implement the regression to predict the number of dengue cases\n","\n","Please save the prediction result in a csv file **hw1_basic.csv**\n"]},{"cell_type":"markdown","metadata":{"id":"RzCR7vk9BFkf"},"source":["## Import Packages\n","\n","> Note: You **cannot** import any other package in the basic part"]},{"cell_type":"code","execution_count":1812,"metadata":{"id":"HL5XjqFf4wSj"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import csv\n","import math\n","import random"]},{"cell_type":"markdown","metadata":{"id":"jnWjrzi0dMPz"},"source":["## Global attributes\n","Define the global attributes"]},{"cell_type":"code","execution_count":1813,"metadata":{"id":"EWLDPOlHBbcK"},"outputs":[],"source":["input_dataroot = 'hw1_basic_input.csv' # Input file named as 'hw1_basic_input.csv'\n","output_dataroot = 'hw1_basic.csv' # Output file will be named as 'hw1_basic.csv'\n","\n","input_datalist =  [] # Initial datalist, saved as numpy array\n","output_datalist =  [] # Your prediction, should be 10 * 4 matrix and saved as numpy array\n","             # The format of each row should be ['epiweek', 'CityA', 'CityB', 'CityC']"]},{"cell_type":"markdown","metadata":{"id":"PsFC-cvqIcYK"},"source":["You can add your own global attributes here\n"]},{"cell_type":"code","execution_count":1814,"metadata":{"id":"OUbS2BEgcut6"},"outputs":[],"source":["training_size = 84\n","testing_size = 10\n","order_temp = 1\n","order_case = 4"]},{"cell_type":"markdown","metadata":{"id":"rUoRFoQjBW5S"},"source":["## Load the Input File\n","First, load the basic input file **hw1_basic_input.csv**\n","\n","Input data would be stored in *input_datalist*"]},{"cell_type":"code","execution_count":1815,"metadata":{"id":"dekR1KnqBtI6"},"outputs":[],"source":["# Read input csv to datalist\n","with open(input_dataroot, newline='') as csvfile:\n","  input_datalist = np.array(list(csv.reader(csvfile)))"]},{"cell_type":"markdown","metadata":{"id":"6kYPuikLCFx4"},"source":["## Implement the Regression Model\n","\n","> Note: It is recommended to use the functions we defined, you can also define your own functions\n"]},{"cell_type":"markdown","metadata":{"id":"jWwdx06JNEYs"},"source":["### Step 1: Split Data\n","Split data in *input_datalist* into training dataset and validation dataset \n","\n"]},{"cell_type":"code","execution_count":1816,"metadata":{"id":"USDciENcB-5F"},"outputs":[],"source":["\n","def SplitData(datalist, start, end, columns_name):\n","    dataset = pd.DataFrame(datalist[start:end], columns = columns_name)\n","    return dataset\n","\n","columns_name = ['epiweek', 'tempA', 'tempB', 'tempC', 'caseA', 'caseB', 'caseC']\n","training_dataset = SplitData(input_datalist, 1, training_size+1, columns_name)\n","# testing_dataset = training_dataset.copy()\n","testing_dataset = SplitData(input_datalist, training_size+1-order_case, training_size+1+testing_size, columns_name)"]},{"cell_type":"markdown","metadata":{"id":"u-3Qln4aNgVy"},"source":["### Step 2: Preprocess Data\n","Handle the unreasonable data\n","> Hint: Outlier and missing data can be handled by removing the data or adding the values with the help of statistics  "]},{"cell_type":"code","execution_count":1817,"metadata":{"id":"XXvW1n_5NkQ5"},"outputs":[],"source":["\n","def PreprocessData(dataset):\n","    # Missing data\n","    missing = (dataset['tempA'] != '') & (dataset['tempB'] != '') & (dataset['tempC'] != '')\n","    dataset.where(missing, other=np.nan, inplace=True)\n","    dataset.dropna(axis='index', inplace=True)\n","\n","    # String to number\n","    for column in dataset:\n","        if column != 'epiweek':\n","            dataset[column] = dataset[column].map(float)\n","\n","    # Outlier for temp\n","    min_temp_A = dataset['tempA'].mean() - 3*dataset['tempA'].std()\n","    max_temp_A = dataset['tempA'].mean() + 3*dataset['tempA'].std()\n","    min_temp_B = dataset['tempB'].mean() - 3*dataset['tempB'].std()\n","    max_temp_B = dataset['tempB'].mean() + 3*dataset['tempB'].std()\n","    min_temp_C = dataset['tempC'].mean() - 3*dataset['tempC'].std()\n","    max_temp_C = dataset['tempC'].mean() + 3*dataset['tempC'].std()\n","    outlierA_temp = (dataset['tempA'] >= min_temp_A) & (dataset['tempA'] <= max_temp_A)\n","    outlierB_temp = (dataset['tempB'] >= min_temp_B) & (dataset['tempB'] <= max_temp_B)\n","    outlierC_temp = (dataset['tempC'] >= min_temp_C) & (dataset['tempC'] <= max_temp_C)\n","    outlier_temp = outlierA_temp & outlierB_temp & outlierC_temp\n","    dataset.where(outlier_temp, other=np.nan, inplace=True)\n","    \n","    # Outlier for case\n","    # min_case_A = dataset['caseA'].mean() - 3*dataset['caseA'].std()\n","    # max_case_A = dataset['caseA'].mean() + 3*dataset['caseA'].std()\n","    # min_case_B = dataset['caseB'].mean() - 3*dataset['caseB'].std()\n","    # max_case_B = dataset['caseB'].mean() + 3*dataset['caseB'].std()\n","    # min_case_C = dataset['caseC'].mean() - 3*dataset['caseC'].std()\n","    # max_case_C = dataset['caseC'].mean() + 3*dataset['caseC'].std()\n","    # outlierA_case = (dataset['caseA'] >= min_case_A) & (dataset['caseA'] <= max_case_A)\n","    # outlierB_case = (dataset['caseB'] >= min_case_B) & (dataset['caseB'] <= max_case_B)\n","    # outlierC_case = (dataset['caseC'] >= min_case_C) & (dataset['caseC'] <= max_case_C)\n","    # outlier_case = outlierA_case & outlierB_case & outlierC_case\n","    # dataset.where(outlier_case, other=np.nan, inplace=True)\n","    \n","    # drop outlier\n","    dataset.dropna(axis='index', inplace=True)\n","    dataset.reset_index(drop=True, inplace=True)\n","    \n","    # Normalize\n","    dataset['tempA'] = dataset['tempA'].map(lambda data: (data-dataset['tempA'].min())/(dataset['tempA'].max()-dataset['tempA'].min()))\n","    dataset['tempB'] = dataset['tempB'].map(lambda data: (data-dataset['tempB'].min())/(dataset['tempB'].max()-dataset['tempB'].min()))\n","    dataset['tempC'] = dataset['tempC'].map(lambda data: (data-dataset['tempC'].min())/(dataset['tempC'].max()-dataset['tempC'].min()))\n","    # dataset['caseA'] = dataset['caseA'].map(lambda data: (data-dataset['caseA'].min())/(dataset['caseA'].max()-dataset['caseA'].min()))\n","    # dataset['caseB'] = dataset['caseB'].map(lambda data: (data-dataset['caseB'].min())/(dataset['caseB'].max()-dataset['caseB'].min()))\n","    # dataset['caseC'] = dataset['caseC'].map(lambda data: (data-dataset['caseC'].min())/(dataset['caseC'].max()-dataset['caseC'].min()))\n","    \n","PreprocessData(training_dataset)\n","PreprocessData(testing_dataset)"]},{"cell_type":"markdown","metadata":{"id":"yDLpJmQUN3V6"},"source":["### Step 3: Implement Regression\n","> Hint: You can use Matrix Inversion, or Gradient Descent to finish this part\n","\n","\n"]},{"cell_type":"code","execution_count":1818,"metadata":{"id":"Tx9n1_23N8C0"},"outputs":[],"source":["def Regression(x, y, learning_rate, learninig_iter, dim):\n","    # weights\n","    w = np.zeros((3, dim))\n","    \n","    # iterate 3 cities\n","    for city in range(3):       \n","        for i in range(learning_iter):\n","            x_trans = x[city].transpose()\n","            \n","            prediction = np.dot(x[city], w[city])\n","            loss = prediction - y[city]\n","            \n","            curr_gradient = np.dot(x_trans, loss) / (N-order_case)\n","            w[city] = w[city] - learning_rate[city] * curr_gradient\n","    \n","    return w\n","    \n","\n","learning_rate = [0.00001, 0.00001, 0.00001]\n","learning_iter = 20000\n","\n","# size of training_dataset\n","N = len(training_dataset)\n","\n","# x for training_data\n","x_temp_train = np.array([\n","    [[(training_dataset['tempA'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, N)],        \n","    [[(training_dataset['tempB'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, N)],        \n","    [[(training_dataset['tempC'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, N)]      \n","])\n","x_case_train = np.array([\n","    [[(training_dataset['caseA'][i-j-1]) for j in range(order_case)] for i in range(order_case, N)],\n","    [[(training_dataset['caseB'][i-j-1]) for j in range(order_case)] for i in range(order_case, N)],\n","    [[(training_dataset['caseC'][i-j-1]) for j in range(order_case)] for i in range(order_case, N)]\n","])\n","x_train = np.array([\n","    np.array([np.concatenate((x_temp_train[0][i], x_case_train[0][i])) for i in range(N-order_case)]),\n","    np.array([np.concatenate((x_temp_train[1][i], x_case_train[1][i])) for i in range(N-order_case)]),\n","    np.array([np.concatenate((x_temp_train[2][i], x_case_train[2][i])) for i in range(N-order_case)])\n","])\n","\n","# y for training_data\n","y_train = np.array([\n","    [training_dataset['caseA'][i] for i in range(order_case, N)],\n","    [training_dataset['caseB'][i] for i in range(order_case, N)],\n","    [training_dataset['caseC'][i] for i in range(order_case, N)]\n","])\n","\n","w = Regression(x_train, y_train, learning_rate, learning_iter, 1+order_temp+order_case)"]},{"cell_type":"markdown","metadata":{"id":"2NxRNFwyN8xd"},"source":["### Step 4: Make Prediction\n","Make prediction of testing dataset and store the value in *output_datalist*"]},{"cell_type":"code","execution_count":1819,"metadata":{"id":"EKlDIC2-N_lk"},"outputs":[{"name":"stdout","output_type":"stream","text":["MAPE_A = 11.9438275090449\n","MAPE_B = 20.46652864044168\n","MAPE_C = 10.36555883515533\n"]}],"source":["def MakePrediction(dataset, x, w, dim):\n","    # MAPE\n","    def MAPE(actual, predict):\n","        return np.mean(np.abs((actual - predict) / actual)) * 100\n","        \n","    # Predict\n","    predict_A = [] \n","    predict_B = [] \n","    predict_C = [] \n","    output_datalist = []\n","    \n","    N = len(dataset)\n","\n","    # x_case for testing_data\n","    x_caseA = [[dataset['caseA'][order_case-j-1] for j in range(order_case)]]\n","    x_caseB = [[dataset['caseB'][order_case-j-1] for j in range(order_case)]]\n","    x_caseC = [[dataset['caseC'][order_case-j-1] for j in range(order_case)]]\n","    \n","    # predict for testing_data\n","    for index in range(N-order_case):\n","        x_A = np.concatenate((x[0][index], x_caseA[index]))\n","        x_B = np.concatenate((x[1][index], x_caseB[index]))\n","        x_C = np.concatenate((x[2][index], x_caseC[index]))\n","        answer_A = sum([w[0][i] * x_A[i] for i in range(dim)])\n","        answer_B = sum([w[1][i] * x_B[i] for i in range(dim)])\n","        answer_C = sum([w[2][i] * x_C[i] for i in range(dim)])\n","        \n","        predict_A.append(round(answer_A))\n","        predict_B.append(round(answer_B))\n","        predict_C.append(round(answer_C))\n","        x_caseA.append([answer_A] + x_caseA[-1][0:-1])\n","        x_caseB.append([answer_B] + x_caseB[-1][0:-1])\n","        x_caseC.append([answer_C] + x_caseC[-1][0:-1])\n","        \n","        output_datalist.append([dataset['epiweek'][index+order_case], round(answer_A), round(answer_B), round(answer_C)])\n","        \n","    # MAPE\n","    actual_A = dataset['caseA'][order_case:].to_numpy()\n","    actual_B = dataset['caseB'][order_case:].to_numpy()\n","    actual_C = dataset['caseC'][order_case:].to_numpy()\n","    \n","    print(\"MAPE_A =\", MAPE(actual_A, predict_A))\n","    print(\"MAPE_B =\", MAPE(actual_B, predict_B))\n","    print(\"MAPE_C =\", MAPE(actual_C, predict_C))\n","\n","    return output_datalist\n","\n","N = len(testing_dataset)\n","# x_temp for testing_data\n","x_temp_test = np.array([\n","    [[(testing_dataset['tempA'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, N)],        \n","    [[(testing_dataset['tempB'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, N)],        \n","    [[(testing_dataset['tempC'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, N)],         \n","])\n","output_datalist = MakePrediction(testing_dataset, x_temp_test, w, 1+order_temp+order_case)"]},{"cell_type":"markdown","metadata":{"id":"cCd0Z6izOCwq"},"source":["### Step 5: Train Model and Generate Result\n","\n","> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n","* If your regression model is *3x^2 + 2x^1 + 1*, your output would be: \n","```\n","3 2 1\n","```\n","\n","\n","\n"]},{"cell_type":"code","execution_count":1820,"metadata":{"id":"iCL92EPKOFIn"},"outputs":[],"source":["# print(f\"{w[0][0]} {w[0][1]} {w[0][2]} {w[0][3]} {w[0][4]} {w[0][5]}\")\n","# print(f\"{w[1][0]} {w[1][1]} {w[1][2]} {w[1][3]} {w[1][4]} {w[1][5]}\")\n","# print(f\"{w[2][0]} {w[2][1]} {w[2][2]} {w[2][3]} {w[2][4]} {w[2][5]}\")"]},{"cell_type":"markdown","metadata":{"id":"J8Jhd8wAOk3D"},"source":["## Write the Output File\n","Write the prediction to output csv\n","> Format: 'epiweek', 'CityA', 'CityB', 'CityC'"]},{"cell_type":"code","execution_count":1821,"metadata":{"id":"tYQVYLlKOtDB"},"outputs":[],"source":["with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n","  writer = csv.writer(csvfile)\n","  for row in output_datalist:\n","    writer.writerow(row)"]},{"cell_type":"markdown","metadata":{"id":"rx4408qg4xMQ"},"source":["# 2. Advanced Part (35%)\n","In the second part, you need to implement the regression in a different way than the basic part to help your predictions for the number of dengue cases\n","\n","We provide you with two files **hw1_advanced_input1.csv** and **hw1_advanced_input2.csv** that can help you in this part\n","\n","Please save the prediction result in a csv file **hw1_advanced.csv** \n"]},{"cell_type":"code","execution_count":1822,"metadata":{"id":"DaZCe19m41g1"},"outputs":[{"name":"stdout","output_type":"stream","text":["MAPE_A = 11.747872095698183\n","MAPE_B = 32.27086600999644\n","MAPE_C = 34.96795610069748\n"]}],"source":["advanced_input1_dataroot = 'hw1_advanced_input1.csv'\n","advanced_input2_dataroot = 'hw1_advanced_input2.csv'\n","advanced_output_dataroot = 'hw1_advanced.csv'\n","\n","advanced_input1_datalist = []\n","advanced_input2_datalist = []\n","advanced_output_datalist = []\n","\n","# Read input csv to datalist\n","with open(advanced_input1_dataroot, newline='') as csvfile:\n","    advanced_input1_datalist = np.array(list(csv.reader(csvfile)))\n","with open(advanced_input2_dataroot, newline='') as csvfile:\n","    advanced_input2_datalist = np.array(list(csv.reader(csvfile)))\n","    \n","# Merge basic_input and advanced_input1\n","advanced_datalist = np.concatenate((input_datalist, advanced_input1_datalist), axis=1)\n","advanced_columns_name = ['epiweek', 'tempA', 'tempB', 'tempC', 'caseA', 'caseB', 'caseC', 'epiweek2', 'precipA', 'precipB', 'precipC']\n","advanced_training_dataset = SplitData(advanced_datalist, 1, training_size+1, advanced_columns_name)\n","advanced_testing_dataset = SplitData(advanced_datalist, training_size+1-order_case, training_size+1+testing_size, advanced_columns_name)\n","advanced_training_dataset.drop(columns='epiweek2', inplace=True)\n","advanced_testing_dataset.drop(columns='epiweek2', inplace=True)\n","\n","# Preproces dataset\n","PreprocessData(advanced_training_dataset)\n","PreprocessData(advanced_testing_dataset)\n","\n","# =================================================================================================\n","\n","# Regression\n","learning_rate = [0.00001, 0.0001, 0.0001]\n","learning_iter = 50000\n","\n","# size of training_dataset\n","N = len(advanced_training_dataset)\n","\n","# x for training_data\n","advanced_x_temp_train = np.array([\n","    [[(advanced_training_dataset['tempA'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, N)],        \n","    [[(advanced_training_dataset['tempB'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, N)],        \n","    [[(advanced_training_dataset['tempC'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, N)]      \n","])\n","advanced_x_precip_train = np.array([\n","    [advanced_training_dataset['precipA'][i] for i in range(order_case, N)],        \n","    [advanced_training_dataset['precipB'][i] for i in range(order_case, N)],        \n","    [advanced_training_dataset['precipC'][i] for i in range(order_case, N)]\n","])\n","advanced_x_case_train = np.array([\n","    [[(advanced_training_dataset['caseA'][i-j-1]) for j in range(order_case)] for i in range(order_case, N)],\n","    [[(advanced_training_dataset['caseB'][i-j-1]) for j in range(order_case)] for i in range(order_case, N)],\n","    [[(advanced_training_dataset['caseC'][i-j-1]) for j in range(order_case)] for i in range(order_case, N)]\n","])\n","advanced_x_train = np.array([\n","    np.array([np.concatenate((advanced_x_temp_train[0][i], [advanced_x_precip_train[0][i]], advanced_x_case_train[0][i])) for i in range(N-order_case)]),\n","    np.array([np.concatenate((advanced_x_temp_train[1][i], [advanced_x_precip_train[1][i]], advanced_x_case_train[0][i])) for i in range(N-order_case)]),\n","    np.array([np.concatenate((advanced_x_temp_train[2][i], [advanced_x_precip_train[2][i]], advanced_x_case_train[0][i])) for i in range(N-order_case)])\n","])\n","\n","advanced_y_train = np.array([\n","    [training_dataset['caseA'][i] for i in range(order_case, N)],\n","    [training_dataset['caseB'][i] for i in range(order_case, N)],\n","    [training_dataset['caseC'][i] for i in range(order_case, N)]\n","])\n","\n","advanced_w = Regression(advanced_x_train, advanced_y_train, learning_rate, learning_iter, 1+order_temp+1+order_case)\n","\n","# =================================================================================================\n","\n","N = len(advanced_testing_dataset)\n","\n","advanced_x_temp_test = np.array([\n","    [[(advanced_testing_dataset['tempA'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, N)],        \n","    [[(advanced_testing_dataset['tempB'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, N)],        \n","    [[(advanced_testing_dataset['tempC'][i]**(order_temp-j)) for j in range(order_temp+1)] for i in range(order_case, N)]      \n","])\n","advanced_x_precip_test = np.array([\n","    [advanced_testing_dataset['precipA'][i] for i in range(order_case, N)],        \n","    [advanced_testing_dataset['precipB'][i] for i in range(order_case, N)],        \n","    [advanced_testing_dataset['precipC'][i] for i in range(order_case, N)]\n","])\n","advanced_x_test = np.array([\n","    np.array([np.concatenate((advanced_x_temp_test[0][i], [advanced_x_precip_test[0][i]])) for i in range(N-order_case)]),\n","    np.array([np.concatenate((advanced_x_temp_test[1][i], [advanced_x_precip_test[1][i]])) for i in range(N-order_case)]),\n","    np.array([np.concatenate((advanced_x_temp_test[2][i], [advanced_x_precip_test[2][i]])) for i in range(N-order_case)])\n","])\n","\n","advanced_output_datalist = MakePrediction(advanced_testing_dataset, advanced_x_test, advanced_w, 1+order_temp+1+order_case)\n","\n","with open(advanced_output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n","  writer = csv.writer(csvfile)\n","  for row in advanced_output_datalist:\n","    writer.writerow(row)"]},{"cell_type":"markdown","metadata":{"id":"EtgCJU7FPeJL"},"source":["# Report *(5%)*\n","\n","Report should be submitted as a pdf file **hw1_report.pdf**\n","\n","*   Briefly describe the difficulty you encountered \n","*   Summarize your work and your reflections \n","*   No more than one page\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hlEE53_MPf4W"},"source":["# Save the Code File\n","Please save your code and submit it as an ipynb file! (**hw1.ipynb**)"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
