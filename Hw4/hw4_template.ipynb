{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IagZMs0_qjdL"
      },
      "source": [
        "# 1. Introduction\n",
        "\n",
        "Welcome to your fourth assignment. In this assignment, you will build a convolutional neural network step by step. In this notebook, you will implement all the functions required to build a convolutional neural network.\n",
        "\n",
        "After finishing this assignment, you will have a deeper understanding of the process of training a convolutional neural network, which mainly consists of two parts: convolution layer and pooling layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGFR00CQvoaH"
      },
      "source": [
        "# 2. Packages\n",
        "All the packages that you need to finish basic part of this assignment are listed below.\n",
        "*   numpy : The fundamental package for scientific computing with Python.\n",
        "*   matplotlib : A comprehensive library for creating static, animated, and interactive visualizations in Python.\n",
        "*   math : Python has a built-in module that you can use for mathematical tasks.\n",
        "*   pandas.read_csv : Provides functionality for reading a csv dataset from a GitHub repository.\n",
        "* sklearn.model_selection.train_test_split: A function helps you split train and test data.\n",
        "* os: A module provides the facility to establish the interaction between the user and the operating system. You can access the image directory by os.\n",
        "* cv2.imread: It is the module import name for opencv-python.\n",
        "* time: Provides various time-related functions.\n",
        "* google.colab.drive: Let you connect colab and your googol drive.\n",
        "* sys: Let you access system-specific parameters and functions.\n",
        "\n",
        "⚠️ **WARNING** ⚠️: \n",
        "*   Please do not import any other packages in basic part.\n",
        "*   np.random.seed(seed) is used to keep all the random function calls consistent. It will help us grade your work. Please don't change the seed.\n",
        "\n",
        "❗ **Important** ❗: Please do not change the code outside this code bracket.\n",
        "```\n",
        "### START CODE HERE ### (≈ n lines)\n",
        "...\n",
        "### END CODE HERE ###\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YcLLrIEc-4h6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/....\") ## the path of the directory where you place dense.py, activation.py ...."
      ],
      "metadata": {
        "id": "_go37iU6-4k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmTH9UkeqdYf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import time \n",
        "import numpy as np\n",
        "from cv2 import imread, IMREAD_GRAYSCALE # IMREAD_GRAYSCALE allow you to load the image as gray scale image\n",
        "from pandas import read_csv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "###### import your HW3 code (Don't change this part) ######\n",
        "from Dense import Dense\n",
        "from Activation import Activation\n",
        "from Loss import compute_BCE_cost\n",
        "from Predict import predict\n",
        "##################################\n",
        "\n",
        "output = {}\n",
        "seed = 1\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMe4BNRPRQvF"
      },
      "source": [
        "# 3. Convolution layer\n",
        "\n",
        "In this section, you will need to implement a very important part of the convolutional neural network, which is the convolution layer. Convolution layer enables us to capture the important features of input images.\n",
        "\n",
        "You will have to implement two helper functions and the forward pass of the convolution layer. All you need to do is to follow the instructions and understand how each part works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADlgENHVRQvG"
      },
      "outputs": [],
      "source": [
        "def zero_pad(X, pad):\n",
        "    \"\"\"\n",
        "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image.\n",
        "    \n",
        "    Argument:\n",
        "    X -- python numpy array of shape (m, n_H, n_W, n_C), where m represent the number of examples.\n",
        "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
        "    \n",
        "    Returns:\n",
        "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
        "    \"\"\"\n",
        "    \n",
        "    # GRADED FUNCTION: zero_padding\n",
        "    ### START CODE HERE ### (≈ 1 line)\n",
        "    X_pad = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return X_pad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNerbFLTRQvG"
      },
      "outputs": [],
      "source": [
        "class Conv():\n",
        "    def __init__(self, filter_size=2, input_channel=3, output_channel=8, pad=1, stride=1, seed=1):\n",
        "        \n",
        "        self.filter_size= filter_size\n",
        "        self.input_channel=input_channel\n",
        "        self.output_channel=output_channel\n",
        "        self.seed = seed\n",
        "        \n",
        "        self.parameters = {\"pad\": pad, \"stride\": stride}\n",
        "        self.initialize_parameters()\n",
        "        \n",
        "        \n",
        "        self.name=\"conv\"\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        \"\"\"\n",
        "        Argument:\n",
        "        self.filter_size -- size of the filter\n",
        "        self.input_channel -- size of the input channel\n",
        "        self.output_channel -- size of the output channel\n",
        "        self.parameters -- python dictionary containing your parameters:\n",
        "                           W -- weight matrix of shape (filter_size, filter_size, input channel size, output channel size)\n",
        "                           b -- bias vector of shape (1, 1, 1, output channel size)\n",
        "                           pad -- amount of padding around each image on vertical and horizontal dimensions\n",
        "                           stride -- represent the amount of movement that a filter move in one step\n",
        "        \"\"\"\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # GRADED FUNCTION: conv_initialization\n",
        "        ### START CODE HERE ### (≈ 8 lines)\n",
        "        None\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        assert(W.shape == (self.filter_size,self.filter_size,self.input_channel,self.output_channel))\n",
        "        assert(b.shape == (1,1,1,self.output_channel))\n",
        "\n",
        "        self.parameters['W'] = W\n",
        "        self.parameters['b'] = b\n",
        "        \n",
        "    \n",
        "    \n",
        "    def conv_single_step(self, a_slice_prev, W, b):\n",
        "        \"\"\"\n",
        "        Apply a filter W on a_slice_prev.\n",
        "\n",
        "        Arguments:\n",
        "        a_slice_prev -- slice of input data of shape (filter_size, filter_size, n_C_prev)\n",
        "        W -- Weight parameters contained in a window - matrix of shape (filter_size, filter_size, n_C_prev)\n",
        "        b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
        "\n",
        "        Returns:\n",
        "        Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
        "        \"\"\"\n",
        "\n",
        "        # GRADED FUNCTION: conv_single_step\n",
        "        ### START CODE HERE ### (≈ 3 lines)\n",
        "        # Element-wise product between a_slice and W.\n",
        "        s = None\n",
        "        # Sum over all entries of the volume s.\n",
        "        Z = None\n",
        "        # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
        "        Z = None\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        return Z\n",
        "    \n",
        "    def forward(self, A_prev):\n",
        "        \"\"\"\n",
        "        Implements the forward propagation for a convolution layer\n",
        "\n",
        "        Arguments:\n",
        "        A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "\n",
        "        Returns:\n",
        "        Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
        "        \"\"\"\n",
        "\n",
        "        # GRADED FUNCTION: conv_forward\n",
        "        ### START CODE HERE ###\n",
        "        # Retrieve dimensions from A_prev's shape (≈1 line)  \n",
        "        (m, n_H_prev, n_W_prev, n_C_prev) = None\n",
        "\n",
        "        # Retrieve dimensions from W's shape (≈1 line)\n",
        "        (f, f, n_C_prev, n_C) = None\n",
        "\n",
        "\n",
        "        # Compute the dimensions of the convolution output volume using the formula given below.(≈2 lines)\n",
        "        n_H = None\n",
        "        n_W = None\n",
        "\n",
        "        # Initialize the output volume Z with zeros. (≈1 line)\n",
        "        Z = None\n",
        "\n",
        "        # if pad!=0, create A_prev_pad by padding A_prev with the parameter \"pad\". (≈1 line)\n",
        "        A_prev_pad = zero_pad(None, None)\n",
        "\n",
        "        for i in range(m):                               # loop over the batch of training examples\n",
        "            a_prev_pad = A_prev_pad[i,:,:,:]               # Select ith training example's padded activation\n",
        "            for h in range(n_H):                           # loop over vertical axis of the output volume\n",
        "                for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
        "                    for c in range(n_C):                   # loop over channels (= #filter) of the output volume\n",
        "\n",
        "                        # Find the corners of the current \"slice\" (≈4 lines)\n",
        "                        None\n",
        "\n",
        "\n",
        "                        # Use the corners to define the slice of a_prev_pad. (≈1 line)\n",
        "                        a_slice_prev = a_prev_pad[ None:None, None:None, :]\n",
        "\n",
        "                        # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)\n",
        "                        Z[i, h, w, c] = self.conv_single_step(None, None, None)\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Making sure your output shape is correct\n",
        "        assert(Z.shape == (m, n_H, n_W, n_C))\n",
        "\n",
        "        # Save information in \"cache\" for the backward pass\n",
        "        self.cache = A_prev\n",
        "\n",
        "        return Z\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        \"\"\"\n",
        "        Implement the backward propagation for a convolution layer\n",
        "\n",
        "        Arguments:\n",
        "        dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
        "\n",
        "        Returns:\n",
        "        dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
        "                   numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        A_prev = self.cache\n",
        "\n",
        "        # Retrieve dimensions from A_prev's shape\n",
        "        (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "\n",
        "        # Retrieve dimensions from W's shape\n",
        "        (f, f, n_C_prev, n_C) = self.parameters[\"W\"].shape\n",
        "\n",
        "\n",
        "        # Retrieve dimensions from dZ's shape\n",
        "        (m, n_H, n_W, n_C) = dZ.shape\n",
        "\n",
        "        # Initialize dA_prev, dW, db with the correct shapes\n",
        "        dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
        "        dW = np.zeros((f, f, n_C_prev, n_C))\n",
        "        db = np.zeros((1, 1, 1, n_C))\n",
        "\n",
        "        # Pad A_prev and dA_prev\n",
        "        A_prev_pad = zero_pad(A_prev, self.parameters[\"pad\"])\n",
        "        dA_prev_pad = zero_pad(dA_prev, self.parameters[\"pad\"])\n",
        "\n",
        "        for i in range(m):                       # loop over the training examples\n",
        "\n",
        "            # select ith training example from A_prev_pad and dA_prev_pad\n",
        "            a_prev_pad = A_prev_pad[i]\n",
        "            da_prev_pad = dA_prev_pad[i]\n",
        "\n",
        "            for h in range(n_H):                   # loop over vertical axis of the output volume\n",
        "                for w in range(n_W):               # loop over horizontal axis of the output volume\n",
        "                    for c in range(n_C):           # loop over the channels of the output volume\n",
        "\n",
        "                        # Find the corners of the current \"slice\"\n",
        "                        vert_start = h * self.parameters[\"stride\"]\n",
        "                        vert_end = vert_start + f\n",
        "                        horiz_start = w * self.parameters[\"stride\"]\n",
        "                        horiz_end = horiz_start + f\n",
        "\n",
        "                        # Use the corners to define the slice from a_prev_pad\n",
        "                        a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
        "\n",
        "                        # Update gradients for the window and the filter's parameters\n",
        "                        da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += self.parameters[\"W\"][:,:,:,c] * dZ[i, h, w, c]\n",
        "                        dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
        "                        db[:,:,:,c] += dZ[i, h, w, c]\n",
        "\n",
        "            # Set the ith training example's dA_prev to the unpaded da_prev_pad\n",
        "            dA_prev[i, :, :, :] = da_prev_pad[self.parameters[\"pad\"]:da_prev_pad.shape[0]-self.parameters[\"pad\"], \n",
        "                                              self.parameters[\"pad\"]:da_prev_pad.shape[1]-self.parameters[\"pad\"], :]\n",
        "\n",
        "        assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
        "\n",
        "        self.dW = dW\n",
        "        self.db = db\n",
        "\n",
        "        return dA_prev\n",
        "    \n",
        "    def update(self, learning_rate):\n",
        "        \"\"\"\n",
        "        Update parameters using gradient descent\n",
        "        \n",
        "        Arguments:\n",
        "        learning rate -- step size\n",
        "        \"\"\"\n",
        "\n",
        "        # GRADED FUNCTION: conv_update\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        self.parameters[\"W\"] = None\n",
        "        self.parameters[\"b\"] = None\n",
        "        ### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1 initialization\n",
        "Using Glorot uniform initialization to initialize the convolution layer's filters with the parameters: filter_size, input_channel, and output_channel.\n",
        "\n",
        "*   Use random initialization (uniform distribution) for the weight matrices. Draws samples from a uniform distribution within [-limit, limit], where limit = sqrt(6 / (fan_in + fan_out)), fan_in is the **number of input channel** and fan_out is the **number of output channel**. However, in the usual implementation, we will consider fan_in and fan_out as the number of input units and the number of output units.\n",
        "*   Use zero initialization for the biases.\n",
        "\n",
        "Exercise: Create and initialize parameters of a convolution layer using Glorot uniform initialization. (1%)\n",
        "\n",
        "It will take following parameters to initialize the convolution layer:\n",
        "*   filter_size: The filter will be in the shape of (filter_size*filter_size)\n",
        "*   input_channel: size of the input channel\n",
        "*   output_channel: size of the output channel\n",
        "*   pad: amount of padding around each image on vertical and horizontal dimensions\n",
        "*   stride: represent the amount of movement that a filter move in one step"
      ],
      "metadata": {
        "id": "3Q_4VjV5W_gL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(seed)\n",
        "conv = Conv(filter_size=2, input_channel=3, output_channel=8, pad=2, stride=2)\n",
        "print(\"W[0][0][0] = \",  conv.parameters[\"W\"][0][0][0])\n",
        "print(\"b = \", conv.parameters[\"b\"])\n",
        "\n",
        "np.random.seed(seed)\n",
        "conv = Conv(filter_size=2, input_channel=3, output_channel=16, pad=2, stride=2)\n",
        "output[\"conv_initialization\"] = conv.parameters[\"W\"][0][0][0]"
      ],
      "metadata": {
        "id": "McQisK1WW-4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>W[0][0][0]: </td>\n",
        "    <td>[-0.12256662  0.32544084 -0.73838    -0.29197414 -0.52177613 -0.6021558 -0.46342438 -0.22812192]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b: </td>\n",
        "    <td>[[[[0. 0. 0. 0. 0. 0. 0. 0.]]]]</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "t50bsMBRTrP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2  Zero-Padding\n",
        "\n",
        "1. It allows you to use a convolution layer without necessarily shrinking the height and width of the volumes. This is important for building deeper networks since otherwise the height/width would shrink as you go to deeper layers. \n",
        "2. It helps us keep more of the information at the border of an image. Without padding, very few values at the next layer would be affected by pixels as the edges of an image.\n",
        "\n",
        "**Exercise**: Implement the zero_pad() function to pad the input X with the given parameter \"pad\". (3%)\n",
        "\n",
        "This function takes the following inputs:\n",
        "*   X: input.\n",
        "*   pad: amount of padding around each image on vertical and horizontal dimensions."
      ],
      "metadata": {
        "id": "OAHuFnmDIhgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(seed)\n",
        "x = np.random.randn(4, 3, 3, 2)\n",
        "x_pad = zero_pad(x, 2)\n",
        "print (\"x.shape =\\n\", x.shape)\n",
        "print (\"x_pad.shape =\\n\", x_pad.shape)\n",
        "print (\"x[0,2,:,0] =\\n\", x[0,2,:,0])\n",
        "print (\"x_pad[0,2,:,0] =\\n\", x_pad[0,2,:,0])\n",
        "\n",
        "fig, axarr = plt.subplots(1, 2)\n",
        "axarr[0].set_title('x')\n",
        "axarr[0].imshow(x[0,:,:,0])\n",
        "axarr[1].set_title('x_pad')\n",
        "axarr[1].imshow(x_pad[0,:,:,0])\n",
        "\n",
        "np.random.seed(seed)\n",
        "x = np.random.randn(4, 2, 2, 2)\n",
        "x_pad = zero_pad(x, 1)\n",
        "output[\"zero_padding\"] = x_pad[0,1,:,0]"
      ],
      "metadata": {
        "id": "h9xGiJH5IgyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>x.shape: </td>\n",
        "    <td>(4, 3, 3, 2)</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>x_pad.shape: </td>\n",
        "    <td>(4, 7, 7, 2)</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>x[0,0,:,0]: </td>\n",
        "    <td>[-0.3224172   1.13376944 -0.17242821]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>x_pad[0,2,:,0]: </td>\n",
        "    <td>[ 0. 0. 1.62434536 -0.52817175 0.86540763 0. 0.]\n",
        "</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "snXOn3sETwDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Convolution_single_step\n",
        "In this part, you will implement a single step of convolution, in which you apply the filter to a single position of the input. This will be used to build a convolutional unit.\n",
        "\n",
        "We will convolve an f*f filter with the image by multiplying its values element-wise with the original matrix, then summing them up and adding a bias.\n",
        "\n",
        "**Exercise**: Implement conv_single_step( ). (5%)\n",
        "\n",
        "This function takes the following inputs:\n",
        "*   a_slice_prev: the output of the activation by the previous layer.\n",
        "*   W: the filter with size f*f.\n",
        "*   b: the bias.\n"
      ],
      "metadata": {
        "id": "GPVifontJlWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(seed)\n",
        "a_slice_prev = np.random.randn(4, 4, 3)\n",
        "W = np.random.randn(4, 4, 3)\n",
        "b = np.random.randn(1, 1, 1)\n",
        "\n",
        "conv = Conv(filter_size=2, input_channel=3, output_channel=8, pad=2, stride=2)\n",
        "Z = conv.conv_single_step(a_slice_prev, W, b)\n",
        "print(\"Z =\", Z)\n",
        "\n",
        "np.random.seed(seed)\n",
        "a_slice_prev = np.random.randn(3, 3, 3)\n",
        "W = np.random.randn(3, 3, 3)\n",
        "b = np.random.randn(1, 1, 1)\n",
        "conv = Conv()\n",
        "Z = conv.conv_single_step(a_slice_prev, W, b)\n",
        "output[\"conv_single_step\"] = Z"
      ],
      "metadata": {
        "id": "02WmPxJKJbJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Z: </td>\n",
        "    <td>[[[-6.99908945]]]</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "SVHY5VIFVLiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Forward pass\n",
        "In the forward pass, you will take many filters and convolve them on the input. Each 'convolution' gives you a 2D matrix output. You will then stack these outputs to get a 3D volume.\n",
        "\n",
        "Notice that the output shape of the convolution forward will be (H, W, C).\n",
        "* $H= \\lfloor\\frac{H_{prev }-f+2*pad}{stride}\\rfloor+1$\n",
        "* $W= \\lfloor\\frac{W_{prev }-f+2*pad}{stride}\\rfloor+1$\n",
        "* $C = $ number of filters\n",
        "\n",
        "f = filter_size\n",
        "\n",
        "**Exercise**: Implement forward( ) to convolve the filters W on an input activation A_prev. (15%)\n",
        "\n",
        "This function takes the following input:\n",
        "*   A_prev: the output of the activation by the previous layer, it's an array with shape (m, H_prev, W_prev, C_prev).\n",
        "    *  m: number of examples.\n",
        "    *  H_prev, W_prev, C_prev: the height, width, and channel of the output of the previous layer.\n",
        "\n",
        "Here are some steps for you to finish this exercise:\n",
        "1. Define **a_slice_prev**, which represent the input slice of conv_single_step( ).\n",
        "2. To define **a_slice_prev**, you have to define its' corners: **vert_start, vert_end, horiz_start and horiz_end**."
      ],
      "metadata": {
        "id": "764-uaZwLGNL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ags0LKKRQvH"
      },
      "outputs": [],
      "source": [
        "np.random.seed(seed) d\n",
        "A_prev = np.random.randn(10,4,4,3)\n",
        "conv=Conv(filter_size=2, input_channel=3, output_channel=8, pad=2, stride=2)\n",
        "Z = conv.forward(A_prev)\n",
        "\n",
        "print(\"Z's mean =\", np.mean(Z))\n",
        "print(\"Z[3,2,1] =\", Z[3,2,1])\n",
        "print(\"cache_conv[1][2][3] =\", conv.cache[1][2][3])\n",
        "\n",
        "\n",
        "np.random.seed(seed)\n",
        "A_prev = np.random.randn(10,3,3,3)\n",
        "conv=Conv(filter_size=3, input_channel=3, output_channel=16, pad=1, stride=1)\n",
        "Z = conv.forward(A_prev)\n",
        "\n",
        "output[\"conv_forward_1\"] = np.mean(Z)\n",
        "output[\"conv_forward_2\"] = Z[3,2,1]\n",
        "output[\"conv_forward_3\"] = conv.cache[1][2][2]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Z's mean: </td>\n",
        "    <td>0.0031904169881830785</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Z[3,2,1]: </td>\n",
        "    <td>[ 1.32947002  2.12083471  0.37853495 -3.53602735  1.38816885 -1.01503137\n",
        " -1.01667531  0.86993377]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>cache_conv[1][2][3]: </td>\n",
        "    <td>[-0.20075807  0.18656139  0.41005165]</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "5qiBeJbhVTlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Update parameters\n",
        "In this section you will update the parameters of the convolution layer, using gradient descent:\n",
        "\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$\n",
        "\n",
        "**Exercise**: Implement update( ) to update your parameters using gradient descent. (1%)\n",
        "\n",
        "**Instructions**: \n",
        "*   Update parameters using gradient descent on $W^{[l]}$ and $b^{[l]}$.\n"
      ],
      "metadata": {
        "id": "bbGLz2F_ReRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv=Conv(kernal_size=2, input_channel=3, output_channel=8, pad=2, stride=2)\n",
        "np.random.seed(seed)\n",
        "conv.dW = np.random.randn(2, 2, 3, 8)\n",
        "conv.db = np.random.randn(1, 1, 1, 8)\n",
        "conv.update(1.0)\n",
        "print(\"W[0][0][0] = \", conv.parameters[\"W\"][0][0][0])\n",
        "print(\"b = \", conv.parameters[\"b\"])\n",
        "\n",
        "conv=Conv(kernal_size=3, input_channel=3, output_channel=8, pad=1, stride=2)\n",
        "np.random.seed(seed)\n",
        "conv.dW = np.random.randn(3, 3, 3, 8)\n",
        "conv.db = np.random.randn(1, 1, 1, 8)\n",
        "conv.update(0.1)\n",
        "output[\"conv_update_1\"] = conv.parameters[\"W\"][0][0][0]\n",
        "output[\"conv_update_2\"] = conv.parameters[\"b\"]\n"
      ],
      "metadata": {
        "id": "QOw8N6q7RgGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>W[0][0][0]: </td>\n",
        "    <td>[-1.74691199  0.93719726 -0.21020825  0.78099448 -1.38718376  1.69938289 -2.20823614  0.53308498]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b: </td>\n",
        "    <td>[[[[ 0.34385368 -0.04359686  0.62000084 -0.69803203  0.44712856\n",
        "    -1.2245077  -0.40349164 -0.59357852]]]]</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "3WBrdsS9RsTA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goYhkmioRQvI"
      },
      "source": [
        "# 4. Maxpooling layer\n",
        "\n",
        "The pooling layer reduces the size (height and width) of the input. It helps reduce computation, as well as helps make feature detectors more invariant to their position in the input. In this section, we will focus on maxpooling layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8j9VErNRQvI"
      },
      "outputs": [],
      "source": [
        "class MaxPool():\n",
        "    def __init__(self, filter_size=2, stride=2):\n",
        "        \"\"\"\n",
        "        Argument:\n",
        "        self.parameters -- python dictionary containing your parameters:\n",
        "                           f -- size of a filter\n",
        "                           stride -- the amount of movement that a filter move in one step\n",
        "        \"\"\"\n",
        "    \n",
        "        self.parameters = {\"f\": filter_size, \"stride\": stride}\n",
        "        self.name=\"maxpool\"\n",
        "        \n",
        "        \n",
        "    def forward(self, A_prev):\n",
        "        \"\"\"\n",
        "        Implements the forward pass of the max pooling layer\n",
        "\n",
        "        Arguments:\n",
        "        A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "\n",
        "        Returns:\n",
        "        A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
        "        \"\"\"\n",
        "\n",
        "        # GRADED FUNCTION: maxpool_forward\n",
        "        ### START CODE HERE ###\n",
        "        # Retrieve dimensions from the input shape. (≈1 line)\n",
        "        (m, n_H_prev, n_W_prev, n_C_prev) = None\n",
        "\n",
        "\n",
        "        # Define the dimensions of the output. (≈3 lines)\n",
        "        n_H = None\n",
        "        n_W = None\n",
        "        n_C = None\n",
        "\n",
        "        # Initialize output matrix A with zeros. (≈1 line)\n",
        "        A = None            \n",
        "\n",
        "        for i in range(m):                         # loop over the training examples\n",
        "            for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
        "                for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
        "                    for c in range (n_C):            # loop over the channels of the output volume\n",
        "\n",
        "                        # Find the corners of the current \"slice\". (≈4 lines)\n",
        "                        None\n",
        "\n",
        "                        # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
        "                        a_prev_slice = A_prev[i, None:None, None:None, None]\n",
        "\n",
        "                        # Compute the max pooling operation on a_prev_slice. (≈1 line)\n",
        "                        A[i, h, w, c] = None\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Store the input in \"cache\" for backward pass\n",
        "        self.cache = A_prev\n",
        "\n",
        "        # Making sure your output shape is correct\n",
        "        assert(A.shape == (m, n_H, n_W, n_C))\n",
        "\n",
        "        return A\n",
        "    \n",
        "    def create_mask_from_window(self, x):\n",
        "        \"\"\"\n",
        "        Creates a mask from an input x to identify the max entry of x.\n",
        "\n",
        "        Arguments:\n",
        "        x -- Array of shape (filter_size, filter_size)\n",
        "\n",
        "        Returns:\n",
        "        mask -- Array of the same shape as filter, contains a True at the position corresponding to the max entry of x.\n",
        "        \"\"\"\n",
        "\n",
        "        mask = x == np.max(x)\n",
        "\n",
        "        return mask\n",
        "    \n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Implements the backward pass of the max pooling layer\n",
        "\n",
        "        Arguments:\n",
        "        dA -- gradient of cost with respect to the output of the pooling layer, same shape as A \n",
        "\n",
        "        Returns:\n",
        "        dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
        "        \"\"\"\n",
        "\n",
        "        # Retrieve information from cache\n",
        "        A_prev = self.cache\n",
        "\n",
        "        # Retrieve dimensions from A_prev's shape and dA's shape\n",
        "        m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
        "        m, n_H, n_W, n_C = dA.shape\n",
        "\n",
        "        # Initialize dA_prev with zeros\n",
        "        dA_prev = np.zeros(A_prev.shape)\n",
        "\n",
        "        for i in range(m):  # loop over the training examples\n",
        "            # select training example from A_prev                    \n",
        "            a_prev = A_prev[i]\n",
        "            for h in range(n_H):   # loop on the vertical axis            \n",
        "                for w in range(n_W):  # loop on the horizontal axis             \n",
        "                    for c in range(n_C): # loop over the channels\n",
        "\n",
        "                        # Find the corners of the current \"slice\"          \n",
        "                        vert_start = h * self.parameters[\"stride\"]\n",
        "                        vert_end = vert_start + self.parameters[\"f\"]\n",
        "                        horiz_start = w * self.parameters[\"stride\"]\n",
        "                        horiz_end = horiz_start + self.parameters[\"f\"] \n",
        "\n",
        "                        #Use the corners and \"c\" to define the current slice from a_prev\n",
        "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
        "                        # Create the mask from a_prev_slice\n",
        "                        mask = self.create_mask_from_window(a_prev_slice)\n",
        "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA)\n",
        "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += np.multiply(mask, dA[i, h, w, c])\n",
        "\n",
        "\n",
        "\n",
        "        # Make sure your output shape is correct\n",
        "        assert(dA_prev.shape == A_prev.shape)\n",
        "\n",
        "        return dA_prev\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Forward pass\n",
        "\n",
        "It will take following parameters to initialize a max pooling layer:\n",
        "*   filter_size: the filter will be in the shape of (filter_size*filter_size)\n",
        "*   stride: the amount of movement that a filter move in one step\n",
        "\n",
        "In the forward pass, you will slide a ( f*f ) filter over the input and store the max value of the window in the output. (f means the filter size)\n",
        "\n",
        "Notice that the output shape of the forward pass will be (H, W, C).\n",
        "* $H= \\lfloor\\frac{H_{prev }-f}{stride}\\rfloor+1$\n",
        "* $W= \\lfloor\\frac{W_{prev }-f}{stride}\\rfloor+1$\n",
        "* $C = C_{prev}$\n",
        "\n",
        "**Exercise**: Create a max pooling layer and implement the forward pass of the pooling layer. (15%)\n",
        "\n",
        "This forward function takes the following input:\n",
        "*   A_prev: the output of the previous layer, it's an array with shape (m, H_prev, W_prev, C_prev).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YXSF-PhOxauK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpL0HQvQRQvJ"
      },
      "outputs": [],
      "source": [
        "np.random.seed(seed)\n",
        "A_prev = np.random.randn(2, 4, 4, 3)\n",
        "maxpool=MaxPool(filter_size=3, stride=2)\n",
        "A = maxpool.forward(A_prev)\n",
        "print(\"A =\", A)\n",
        "\n",
        "A_prev = np.random.randn(2, 5, 5, 3)\n",
        "maxpool=MaxPool(filter_size=2, stride=1)\n",
        "A = maxpool.forward(A_prev)\n",
        "output[\"maxpool_forward\"] = A"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>A: </td>\n",
        "    <td>[[[[1.74481176 0.86540763 1.13376944]]] [[[1.13162939 1.51981682 2.18557541]]]]</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "9vcEzFinVYHP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn-VBGGURQvJ"
      },
      "source": [
        "# 5. Flatten layer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To connect the convolution layer and the dense layer, you should flatten the output of the convolution layer or max pooling layer before dense layer."
      ],
      "metadata": {
        "id": "cJN7EvSuaKGW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-F3jskjRQvK"
      },
      "outputs": [],
      "source": [
        "class Flatten():\n",
        "    def __init__(self):\n",
        "        self.name=\"flatten\"\n",
        "\n",
        "    def forward(self, A_prev):\n",
        "        \"\"\"\n",
        "        Implements the forward pass of the flatten layer\n",
        "\n",
        "        Arguments:\n",
        "        A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "\n",
        "        Returns:\n",
        "        A -- output of the flatten layer, a 1-dimensional array\n",
        "        \"\"\"\n",
        "\n",
        "        # Save information in \"cache\" for the backward pass\n",
        "        self.cache = A_prev[0].shape\n",
        "\n",
        "        # GRADED FUNCTION: flatten_forward\n",
        "        ### START CODE HERE ### (≈1 line)\n",
        "        A = None\n",
        "        ### END CODE HERE ###\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Implements the backward pass of the flatten layer\n",
        "\n",
        "        Arguments:\n",
        "        dA -- Input data, a 1-dimensional array\n",
        "\n",
        "        Returns:\n",
        "        dA_prev -- An array with its original shape (the output shape of its' previous layer).\n",
        "        \"\"\"\n",
        "        # GRADED FUNCTION: flatten_backward\n",
        "        ### START CODE HERE ### (≈1 line)\n",
        "        dA_prev = None\n",
        "        ### END CODE HERE ###\n",
        "        return dA_prev"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Forward pass\n",
        "\n",
        "**Exercise**: Implement the forward pass of flatten layer. Turn the input array into a 1-dimensional array. (5%)\n",
        "\n",
        "This function takes the following input:\n",
        "*   A_prev: Input data, it's an array with shape (m, n_H_prev, n_W_prev, n_C_prev).\n"
      ],
      "metadata": {
        "id": "5Haf0l4nau3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(seed)\n",
        "A_prev = np.random.randn(2,2,2,2)\n",
        "flatten = Flatten()\n",
        "A = flatten.forward(A_prev)\n",
        "print(\"A.shape =\", A.shape)\n",
        "print(\"A[0] =\", A[0])\n",
        "\n",
        "\n",
        "np.random.seed(seed)\n",
        "A_prev = np.random.randn(2,3,3,2)\n",
        "flatten = Flatten()\n",
        "A = flatten.forward(A_prev)\n",
        "output[\"flatten_forward\"] = A[0]"
      ],
      "metadata": {
        "id": "TF96C0Fyat_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>A.shape: </td>\n",
        "    <td>(2, 8)</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>A[0]: </td>\n",
        "    <td>[ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.3015387\n",
        "  1.74481176 -0.7612069 ]</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "Rq3qbOjiVhjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2  Backward pass\n",
        "\n",
        "**Exercise**: Implement the backward pass of flatten layer. Turn the input array back to its original shape.(the output shape of its' previous layer). (5%)\n",
        "\n",
        "This function takes the following input:\n",
        "*   dA: the output of backward pass from the next layer, it's a 1-dimensional array."
      ],
      "metadata": {
        "id": "qdWPIB6_a8_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(seed)\n",
        "A_prev = np.random.randn(2,2,2,2)\n",
        "flatten = Flatten()\n",
        "A = flatten.forward(A_prev)\n",
        "B = flatten.backward(A)\n",
        "print(\"B.shape =\", B.shape)\n",
        "print(\"B[0] =\", B[0])\n",
        "\n",
        "# B and A_prev should be same\n",
        "assert((B==A_prev).all())\n",
        "\n",
        "np.random.seed(seed)\n",
        "A_prev = np.random.randn(4,3,3,3)\n",
        "flatten = Flatten()\n",
        "A = flatten.forward(A_prev)\n",
        "B = flatten.backward(A)\n",
        "output[\"flatten_backward\"] = B[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "dY8vpJPLauWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>B.shape: </td>\n",
        "    <td>(2, 2, 2, 2)</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>B[0]: </td>\n",
        "    <td>[[[ 1.62434536 -0.61175641]\n",
        "  [-0.52817175 -1.07296862]]\n",
        "  [[ 0.86540763 -2.3015387 ]\n",
        "  [ 1.74481176 -0.7612069 ]]]</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "3khMiPehVjIV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYqpQu6Eye7h"
      },
      "source": [
        "# 6. Model\n",
        "Alright, now you have all the tools that are needed to build a convolutional neural network. Let's get started! Use the knowledge you learned from assignment 4 to finish this part. But there is some difference:\n",
        "\n",
        "1. In this part, we will call model.add( ) to add a layer into the model. For example:\n",
        "* model.add(Conv( )): add a convolution layer into the model.\n",
        "* model.add(Dense( )): add a dense layer into the model.\n",
        "* model.add(Activation( )): add an activation layer into the model.\n",
        "\n",
        "2. Because the dense layer you implement in assignment3 takes the input shape as [:, m], where m represents the number of examples. However, when the training data go through the convolution layer and maxpool layer, its shape will be [m,:]. As a consequence, in the forward pass, the output of flatten.forward( ) need to be transposed. Similarly, in the backward pass, before the data goes into flatten.backward( ), it needs to be transposed again.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dWrCCkPRQvK"
      },
      "outputs": [],
      "source": [
        "class Model():\n",
        "    def __init__(self):       \n",
        "        self.layers=[]\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, X):\n",
        "        A = X\n",
        "        # GRADED FUNCTION: model\n",
        "        ### START CODE HERE ### (≈ 5 lines)\n",
        "        for l in range(len(self.layers)):\n",
        "            if(self.layers[l].name==\"flatten\"):\n",
        "                A=self.layers[l].forward(A).T # Transpose after flatten layer\n",
        "            else:\n",
        "                None\n",
        "        ### END CODE HERE ###\n",
        "        return A\n",
        "\n",
        "    def backward(self, AL=None, Y=None):\n",
        "        L = len(self.layers)\n",
        "\n",
        "        # GRADED FUNCTION: model\n",
        "        ### START CODE HERE ### (≈ 7 lines)\n",
        "        if self.layers[-1].name == \"sigmoid\":\n",
        "            dAL = None\n",
        "            dZ = self.layers[-1].backward(dA=dAL)  #activation layer backward\n",
        "            dA_prev = self.layers[-2].backward(dZ) #linear layer backward\n",
        "        else:\n",
        "            dZ = self.layers[-1].backward(Y=Y)\n",
        "            dA_prev = self.layers[-2].backward(dZ)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        \n",
        "        # Loop from l=L-3 to l=0\n",
        "        # GRADED FUNCTION: model\n",
        "        ### START CODE HERE ### (≈ 5 lines)\n",
        "        for l in reversed(range(L-2)):\n",
        "            if(self.layers[l].name==\"flatten\"):\n",
        "                dA_prev=self.layers[l].backward(dA_prev.T) # Transpose before goes into flatten layer\n",
        "            else:\n",
        "                dA_prev=None\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        return dA_prev\n",
        "\n",
        "    def update(self, learning_rate):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "        learning_rate -- step size\n",
        "        \"\"\"\n",
        "        \n",
        "        # GRADED FUNCTION: model\n",
        "        # Only convolution layer and dense layer have to update parameters\n",
        "        ### START CODE HERE ### (≈ 3 lines)\n",
        "        None\n",
        "        ### END CODE HERE ###\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 Model forward, backward and update:\n",
        "**Exercise**: Here is an exercise to make sure your model works correctly. (5%)"
      ],
      "metadata": {
        "id": "36my0zWnlv3K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN-8NQ_KRQvK"
      },
      "outputs": [],
      "source": [
        "np.random.seed(seed)\n",
        "A = np.random.randn(4,10,10,3)\n",
        "Y = np.array([[1,0,1,0]])\n",
        "\n",
        "model=Model()\n",
        "model.add(Conv(filter_size=3, input_channel=3, output_channel=8, pad=1, stride=2))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPool(filter_size=2, stride=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, 1))\n",
        "model.add(Activation(\"sigmoid\"))\n",
        "\n",
        "\n",
        "AL = model.forward(A)\n",
        "dA_prev = model.backward(AL=AL, Y=Y)\n",
        "model.update(0.01)\n",
        "\n",
        "print(model.layers[0].dW[0,0,0])\n",
        "print(model.layers[0].db)\n",
        "print(model.layers[4].dW[0,:8])\n",
        "print(model.layers[4].db)\n",
        "\n",
        "\n",
        "np.random.seed(seed)\n",
        "A = np.random.randn(4,8,8,3)\n",
        "Y = np.array([[1,1,0,0]])\n",
        "\n",
        "model=Model()\n",
        "model.add(Conv(filter_size=3, input_channel=3, output_channel=16, pad=1, stride=2))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPool(filter_size=2, stride=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, 1))\n",
        "model.add(Activation(\"sigmoid\"))\n",
        "\n",
        "\n",
        "AL = model.forward(A)\n",
        "dA_prev = model.backward(AL=AL, Y=Y)\n",
        "model.update(0.001)\n",
        "\n",
        "output[\"model_1\"] = model.layers[0].dW[0,0,0]\n",
        "output[\"model_2\"] = model.layers[0].db\n",
        "output[\"model_3\"] = model.layers[4].dW[0,:8]\n",
        "output[\"model_4\"] = model.layers[4].db"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>model.layers[0].dW[0,0,0]: </td>\n",
        "    <td>[ 0.36135339 -0.08462337 -0.00125603 -0.75846791 -0.10766644 -0.30566005\n",
        " -0.6160899   0.17289454]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>model.layers[0].db: </td>\n",
        "    <td>[[[[-0.17413437 -1.16136976  2.2091218   1.09197293  1.09878206\n",
        "    -0.99630691  2.22696487 -0.20973624]]]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>model.layers[4].dW[0,:8]: </td>\n",
        "    <td>[-2.14606176 -0.75085187 -1.19750975 -0.8916535  -0.91436404 -0.76753\n",
        " -1.30207298 -0.52670234]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>model.layers[4].db: </td>\n",
        "    <td>[[-0.47493517]]</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "vGTtpnWcVvce"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EC0qy26RQvN"
      },
      "source": [
        "# 7. Binary classification\n",
        "\n",
        "Congratulations on implementing all the functions by yourself. You have done an incredible job! 👏\n",
        "\n",
        "Now you have all the tools you need to get started with classification. In this section, you will build a binary classifier using the functions you had previously written. You will create a model that can determine whether a CXR image is normal or not. There will be 600 training images and 60 testing images, and the size of all images are 32 * 32 * 1.\n",
        "\n",
        "\n",
        "**Exercise**: Implement a binary classifier and tune the hyperparameter. You will get all 10% if your prediction achieves accuracy greater than 0.55 in testing data. (10%)\n",
        "\n",
        "**Instruction**:\n",
        "*   You can only use the functions you had previously written.\n",
        "*   Preprocess the data by using min-max scaling to normalize X. Normalize the values of each feature between 0 and 1.\n",
        "*   Use batch gradient descent to train the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1 Data preprocessing"
      ],
      "metadata": {
        "id": "K2X2fb7aoJTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"Training_data\"  #path to your training image\n",
        "file_dir = os.listdir(PATH) #read the images from the directory\n",
        "file_dir.sort() #Make sure the images are loaded in order\n",
        "X_train = np.array([])\n",
        "\n",
        "# Prepare X_train\n",
        "# The shape of X_train will be (number of examples, height of image, width of image, channel of image)\n",
        "# GRADED CODE: Binary classification (Data preprocessing)\n",
        "# hint: use imread(PATH, IMREAD_GRAYSCALE) to load image\n",
        "### START CODE HERE ### (≈ 9 line)\n",
        "\n",
        "\n",
        "### END CODE HERE ###\n",
        "        \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QWHxt5cDZ9mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"Testing_data\"  #path to your testing image\n",
        "file_dir = os.listdir(PATH)\n",
        "file_dir.sort()\n",
        "X_test = np.array([])\n",
        "\n",
        "# Prepare X_test\n",
        "# The shape of X_teset will be (number of examples, height of image, width of image, channel of image)\n",
        "# GRADED CODE: Binary classification (Data preprocessing)\n",
        "### START CODE HERE ### (≈ 9 line)\n",
        "\n",
        "\n",
        "### END CODE HERE ###\n",
        "        \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I527tbN2_Obg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = read_csv(\"Training_label.csv\")\n",
        "\n",
        "y_train = []\n",
        "# Prepare y_train\n",
        "# The shape of y_train will be (number of examples, 1), we will transpose y_train latter.\n",
        "# GRADED CODE: Binary classification (Data preprocessing)\n",
        "### START CODE HERE ### (≈ 2 line)\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "bxaslew_lkKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#You can split training and validation set here. (Optional)\n",
        "### START CODE HERE ###\n",
        "\n",
        "### END CODE HERE ###"
      ],
      "metadata": {
        "id": "iwhza4gUboQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train.T #transpose y_train\n",
        "\n",
        "# plot first few images\n",
        "for i in range(9):\n",
        "    # define subplot\n",
        "    plt.subplot(330 + 1 + i)\n",
        "    # plot raw pixel data\n",
        "    plt.imshow(X_train[i].squeeze(), cmap='gray', vmin=0, vmax=1)\n",
        "# show the figure\n",
        "plt.show()\n",
        "\n",
        "# check the shape of training data and testing data\n",
        "print('Train: X=%s, y=%s' % (X_train.shape, y_train.shape))\n",
        "print('Test: X=%s' % (X_test.shape))"
      ],
      "metadata": {
        "id": "wKkOpp6uyCQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2 mini-batch gradient descent"
      ],
      "metadata": {
        "id": "qdK9_gnZAjYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
        "    \"\"\"\n",
        "    Creates a list of random minibatches from (X, Y)\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, of shape !!!!!!!!!!!(number of examples ,input size)!!!!!!!!!!!\n",
        "    Y -- true \"label\" vector, of shape (number of classes, number of examples)\n",
        "    mini_batch_size -- size of the mini-batches, integer\n",
        "    \n",
        "    Returns:\n",
        "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[0]  # number of training examples\n",
        "    mini_batches = []\n",
        "\n",
        "    # GRADED CODE: Binary classification\n",
        "    ### START CODE HERE ###\n",
        "        \n",
        "    # Step 1: Shuffle (X, Y)\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = None\n",
        "    shuffled_Y = None\n",
        "    \n",
        "    inc = mini_batch_size\n",
        "\n",
        "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
        "    # Cases with a complete mini batch size only i.e each of 64 examples.\n",
        "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "        # (approx. 2 lines)\n",
        "        mini_batch_X = None\n",
        "        mini_batch_Y = None\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
        "    if m % mini_batch_size != 0:\n",
        "        #(approx. 2 lines)\n",
        "        mini_batch_X = None\n",
        "        mini_batch_Y = None\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    \n",
        "    return mini_batches\n",
        "\n",
        "    ### END CODE HERE ###"
      ],
      "metadata": {
        "id": "LVTeqK9TqMwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3 Start training"
      ],
      "metadata": {
        "id": "5FIrnqYMFGRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED CODE: Binary classification\n",
        "### START CODE HERE ###\n",
        "learning_rate = 3e-5\n",
        "num_iterations = 20\n",
        "batch_size = 16\n",
        "print_cost = True\n",
        "classes = 2\n",
        "costs = []   # keep track of cost\n",
        "\n",
        "\n",
        "# build the model\n",
        "model=Model()\n",
        "model.add(Conv(filter_size=3, input_channel=1, output_channel=16, pad=0, stride=2))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPool(filter_size=2, stride=2))\n",
        "model.add(Conv(filter_size=3, input_channel=16, output_channel=16, pad=0, stride=2))\n",
        "model.add(Activation(\"relu\"))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(144, 32))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dense(32, 1))\n",
        "model.add(Activation(\"sigmoid\"))\n",
        "\n",
        "# Loop (gradient descent)\n",
        "for i in range(0, num_iterations):\n",
        "    print(\"epoch: \",i)\n",
        "    mini_batches = random_mini_batches(X_train, y_train, batch_size)\n",
        "    j=0\n",
        "    for batch in mini_batches:\n",
        "        x_batch, y_batch = batch\n",
        "\n",
        "        # forward\n",
        "        AL = None\n",
        "\n",
        "        # compute cost\n",
        "        if classes == 2:\n",
        "            cost = None\n",
        "        else:\n",
        "            cost = None\n",
        "\n",
        "        # backward\n",
        "        dA_prev = None\n",
        "\n",
        "        # update\n",
        "        None\n",
        "    \n",
        "    print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    costs.append(cost)\n",
        "### END CODE HERE ###\n",
        "            \n",
        "# plot the cost\n",
        "plt.plot(np.squeeze(costs))\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1CBktduDyKd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test = predict(X_test, None, model, 2)\n",
        "output[\"basic_pred_test\"] = pred_test[0].astype(int)\n",
        "\n",
        "basic_model_layers = []\n",
        "basic_model_parameters = []\n",
        "for layer in model.layers:\n",
        "    basic_model_layers.append(layer.name)\n",
        "    if(layer.name==\"conv\" or layer.name==\"dense\"):\n",
        "        basic_model_parameters.append(layer.parameters)\n",
        "output[\"basic_model_layers\"] = basic_model_layers\n",
        "output[\"basic_model_parameters\"] = basic_model_parameters"
      ],
      "metadata": {
        "id": "1hIZQOfGAPJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Advanced implementation\n",
        "In this part, you will practice how to build a model by **Tensorflow**, and you finally can use GPU to accelerate the training process☺️. You can import any packages in the advanced part.\n",
        "\n",
        "**Exercise**: Implement a binary classifier by Tensorflow. You will get 15% if your prediction achieves accuracy greater than 0.7 in testing data. The rest 10% will be graded by your rank.\n",
        "\n",
        "Except you have to build the model by Tensorflow, there is no limitation in this part. You can try different model architectures, optimization parametes and image augmentation methods to get good performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "rrrYp1RwSIOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import some tensorflow packages to help you build the model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# import other packages here"
      ],
      "metadata": {
        "id": "PDyxdy9Jk982"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED CODE: Advanced implementation\n",
        "### Data preprocess & augmentation ###\n",
        "#You may have to adjust the shape of y_train"
      ],
      "metadata": {
        "id": "hhAjQTjPk-I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED CODE: Advanced implementation\n",
        "### Start training ###"
      ],
      "metadata": {
        "id": "PgL3loUQk-LO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test = model.predict(X_test)\n",
        "pred_test = np.argmax(pred_test, axis = 1)\n",
        "output[\"advanced_pred_test\"] = pred_test"
      ],
      "metadata": {
        "id": "pObMJ67hk-Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXGnS3HQeNUc"
      },
      "source": [
        "# Submit prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twMsmXbQeDL_"
      },
      "outputs": [],
      "source": [
        "# sanity check\n",
        "assert(list(output.keys()) == ['conv_initialization', 'zero_padding', 'conv_single_step', 'conv_forward_1', 'conv_forward_2', 'conv_forward_3', 'maxpool_forward', 'flatten_forward', 'flatten_backward', 'model_1', 'model_2', 'model_3', 'model_4', 'basic_pred_test', 'basic_model_layers', 'basic_model_parameters', 'advanced_pred_test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCJ0XTO_zE8A"
      },
      "outputs": [],
      "source": [
        "np.save(\"hw4_output.npy\", output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFBFUUEg1to-"
      },
      "outputs": [],
      "source": [
        "# sanity check\n",
        "submit = np.load(\"hw4_output.npy\", allow_pickle=True).item()\n",
        "for key, value in submit.items():\n",
        "    print(str(key) + \"： \" + str(type(value)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBkBtZHxIh8Z"
      },
      "source": [
        "Expected output:<br>\n",
        "<small>\n",
        "conv_initialization： <class 'numpy.ndarray'> <br>\n",
        "zero_padding： <class 'numpy.ndarray'> <br>\n",
        "conv_single_step： <class 'numpy.ndarray'> <br>\n",
        "conv_forward_1： <class 'numpy.float64'> <br>\n",
        "conv_forward_2： <class 'numpy.ndarray'> <br>\n",
        "conv_forward_3： <class 'numpy.ndarray'> <br>\n",
        "conv_update_1： <class 'numpy.ndarray'> <br>\n",
        "conv_update_2： <class 'numpy.ndarray'> <br>\n",
        "maxpool_forward： <class 'numpy.ndarray'> <br>\n",
        "flatten_forward： <class 'numpy.ndarray'> <br>\n",
        "flatten_backward： <class 'numpy.ndarray'> <br>\n",
        "model_1： <class 'numpy.ndarray'> <br>\n",
        "model_2： <class 'numpy.ndarray'> <br>\n",
        "model_3： <class 'numpy.ndarray'> <br>\n",
        "model_4： <class 'numpy.ndarray'> <br>\n",
        "basic_pred_test： <class 'numpy.ndarray'> <br>\n",
        "basic_model_layers： <class 'list'> <br>\n",
        "basic_model_parameters： <class 'list'> <br>\n",
        "advanced_pred_test： <class 'numpy.ndarray'> <br>\n",
        "</small>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "764-uaZwLGNL",
        "bbGLz2F_ReRr",
        "YXSF-PhOxauK",
        "5Haf0l4nau3y",
        "qdWPIB6_a8_n",
        "36my0zWnlv3K",
        "qdK9_gnZAjYD",
        "5FIrnqYMFGRq"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}